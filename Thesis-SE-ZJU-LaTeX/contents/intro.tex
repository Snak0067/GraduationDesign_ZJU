% !TEX root = ../main.tex

% 第一章一般名为绪论／引言，不可省略

\chapter{绪论}

\section{课题背景与研究意义}

在当前人工智能技术广泛赋能的时代，结构化图数据\cite{图结构数据1,图结构数据2}在多个关键领域中扮演着基础性角色。
社交网络中用户间的关注、互动构成异质社交图，用于社交网络分析\cite{社交网络分析1,社交网络分析2}与推荐系统 \cite{He2020LightGCNSA,Wang2021LearningIB}；
生物医药领域中分子结构与蛋白质交互天然以图形式建模，广泛用于生物网络分析\cite{Guo2022GraphbasedMR,Liu_Wang_Vu_Moretti_Bodenheimer_Meiler_Derr_2023}与分子生成\cite{Hu2020OpenGB}；
交通调度与城市计算将路网与交通流抽象为图模型，用于路径优化与实时控制\cite{Zheng2019GMANAG}；
金融风控利用交易网络进行欺诈检测和信用建模\cite{eFraudCom}；
电子电路与EDA设计中电路拓扑本身即为图结构\cite{DeepGate}。
随着多领域数据规模与复杂性不断增长，图结构在表示复杂实体间多关系、高阶依赖方面展现出不可替代的表达能力。
因此，如何从图结构中高效抽取语义、理解拓扑规律，并迁移到各类图计算任务中，成为当前人工智能研究中的重要课题之一。

传统的图结构计算
主要依赖符号方法\cite{Blondel2008FastUO,随机游走,谱聚类,标签传播,图同构检测}与
统计学习方法\cite{Perozzi2014DeepWalkOL,Grover2016node2vecSF,Tang2015LINELI,高阶邻接嵌入,有向图高阶传递性嵌入}。
符号方法以图论与谱分析为核心，
通过模块度优化\cite{Blondel2008FastUO}、随机游走\cite{随机游走}、谱聚类\cite{谱聚类}、标签传播\cite{标签传播}及图同构检测\cite{图同构检测}等手段揭示图的拓扑规律，具有良好的可解释性，
但在处理大规模、动态与异质图时受限于计算复杂度与人工特征设计。
统计学习方法则通过随机游走\cite{Perozzi2014DeepWalkOL,Grover2016node2vecSF}与矩阵分解\cite{Tang2015LINELI,高阶邻接嵌入,有向图高阶传递性嵌入}等嵌入技术，
将离散图结构映射至连续向量空间，以实现节点、边及子图的低维表征。
然而，这些传统范式普遍存在局部表达受限，在大规模图计算中面临效率与泛化性挑战。

随着图神经网络（GNNs）的发展，图学习领域逐步从传统的机器学习转向深度学习。
然而，过平滑现象（over-smoothing）\cite{Rusch2023ASO}和过压缩问题（over-squashing）\cite{Alon2020OnTB}问题共同限制了 GNNs 在大规模图数据上的表达能力与可扩展性。
为突破这一限制，研究者开始探索以大规模语言模型（如BERT\cite{BERT}, T5\cite{T5}, LLaMA\cite{Llama}）为核心的图结构计算新范式。
该方向旨在将图的结构关系与语义信息统一映射至语言空间，使模型具备跨节点、跨关系的全局推理能力。
依托语言模型卓越的语义理解与生成式推理机制，图数据获得了语义补全、自解释与可迁移的潜能，展现出超越传统图神经网络的泛化能力与知识整合能力。
在这一趋势下，图大语言模型（Graph Large Language Models,Graph-LLMs） 逐渐成为连接符号推理与语义建模的关键桥梁。它们试图让语言模型具备理解、推理并生成图结构信息的能力，从而在统一的语义框架中实现图计算与自然语言处理的融合。

尽管 Graph-LLMs 在图语义理解与推理方面取得初步进展，现有研究仍面临一些技术性挑战和局限性，主要集中在以下几个方面：

\textbf{1）图结构编码存在信息缺失与拓扑保真不足：} 现有做法多依赖邻域采样与简化线性化，将图转换为可读序列或片段式子图。此过程易引入起点与遍历路径偏置，边与连通性被截断或重排，环、桥接与长链路等关键结构难以被完整呈现。随之而来的，是对路径可达性、中心性与全局属性的估计偏差，导致在节点分类、链接预测与结构问答等任务上出现系统性性能下滑。

\textbf{2）表示保真与跨模态对齐不足：} 将图转写为文本后，结构信号被弱化为局部片段，缺乏对多阶邻居与全局依赖的统一刻画；同时，语言模型天然缺少图归纳偏置，图与文本嵌入空间难以实现稳定的一一对应。结果表现为：全局与局部语义映射不一致，节点/边级细粒度语义难以锚定到相应词片，跨样本与跨域的表示漂移加剧，模型对复杂拓扑的理解与迁移显著受限。

\textbf{3）推理建模与泛化能力不足：} 以监督微调为主的训练范式往往学习任务捷径而非通用结构规律，模型多给出“结论式”回答而缺少可核验的中间推理链，导致可解释性与稳健性不足。面对未见任务、异构图或尺度变化时，表现出明显的域外退化：对长距离依赖的判断不稳定，对结构扰动与数据漂移敏感，零样本与小样本条件下的迁移能力有限。

尽管存在上述挑战，近年来深度学习技术的演进，尤其是跨模态表征学习与生成式预训练范式的发展，为图大语言模型的进一步突破奠定了基础。
一方面，多模态融合模型在视觉、语音、文本等领域的成功实践表明，大模型具备强大的跨域语义抽象与表示迁移能力，为图模态与语言模态的统一建模提供了新的范式支撑。
另一方面，图结构序列化编码、对比学习与指令微调等方法的引入，使语言模型能够在结构感知的前提下学习图的拓扑依赖与语义关联，提升模型在复杂图任务中的理解与推理能力。

\section{相关研究工作}

对于国内硕士学位论文来说，
一般较少研究完全无前人探索的领域，
所以有必要交待前人在此做出的努力和尝试。
同样，请提供数据和引用保证严谨。

为避免引起评阅老师判定有凑篇幅之嫌，
请有针对的描述前人研究的不足之处，
做到``有破有立''。
\subsection{基于图神经网络的方法}

长期以来，图神经网络（Graph Neural Networks, GNN）\cite{Kipf2016SemiSupervisedCW,Wu2019ACS,Han2023PiVePW,Wu2019SimplifyingGC,Wang2021BagOT}
一直是图机器学习的主导范式。GNN 通过消息传递与邻域聚合机制，在统一框架下同时建模节点特征与图拓扑结构，为各类图表示学习任务提供了基础工具。

从模型设计范式来看，基于不同的消息传递与卷积定义方式，主流 GNN 模型大致可以分为谱域方法和空间域方法两大类。
谱域方法以图拉普拉斯谱理论为基础，在频域中定义卷积核再映射回节点空间。Defferrard 等人\cite{Defferrard2016ConvolutionalNN}提出的 ChebNet
通过切比雪夫多项式对谱卷积进行局部近似，显著降低了谱滤波的计算成本；
Kipf 和 Welling等人\cite{Kipf2016SemiSupervisedCW} 提出的图卷积网络（Graph Convolutional Network, GCN）
在 ChebNet 的基础上进一步采用一阶谱近似，形成简单而高效的“归一化邻接矩阵 + 线性变换”结构，被广泛视为谱域 GNN 的里程碑工作；
随后，Wu 等人\cite{Wu2019SimplifyingGC}从信号平滑视角对 GCN 进行简化，揭示了多层 GCN 本质上是一种低通滤波器，为后续关于过平滑现象的研究奠定了理论基础。

与之相对，空间域方法直接在图的邻接结构上定义局部消息传递规则，通过聚合节点一阶或多阶邻域特征完成表示更新。
Hamilton 等人\cite{Hamilton2017InductiveRL}提出的 GraphSAGE
针对 GCN 难以处理未见节点的问题，构建了“邻域采样 + 特征聚合”的归纳式框架，支持在大规模动态图上进行在线推断；
Veličković 等人\cite{Velickovic2017GraphAN}提出的图注意力网络（Graph Attention Network, GAT）
将自注意力机制引入邻域聚合，通过学习可变权重区分不同邻居的重要性，缓解了简单平均聚合对噪声节点与不相关邻居的过度平滑问题；
在此基础上，Brody 等人\cite{Brody2021HowAA}指出原始 GAT 的注意力打分实质上是“静态”的，难以捕捉特征变换后的高阶依赖关系，并提出动态注意力变体以提升表达灵活性。
Chen 等人\cite{Chen2021EdgeFeaturedGA}进一步沿注意力方向扩展，提出显式建模边特征的 Edge-featured GAT（EGAT），
通过对节点和边分别构建注意力通道、在多层结构中进行多尺度融合，使 GNN 能够在节点与边两个层面同时学习表征。

在提升表达能力方面，Xu 等人\cite{Xu2018HowPA}提出的图同构网络（Graph Isomorphism Network, GIN）
通过在聚合时显式引入自环并采用求和聚合，在理论上证明其判别能力与 Weisfeiler–Lehman 图同构测试等价，显著增强了模型对局部子结构模式的区分能力。
Monti 等人\cite{Monti2016GeometricDL}则从几何深度学习视角出发，将图卷积推广至一般流形与不规则网格，通过构造局部坐标系与核函数，在统一框架下处理社交网络、交通网络等空间图数据。

尽管取得了大量进展，GNN 也暴露出一系列固有局限。
一方面，随着网络层数加深，节点表征逐渐趋于同质化，即所谓“过平滑”现象\cite{Rusch2023ASO}，导致不同类别或结构位置的节点难以区分；
另一方面，多跳依赖在固定维度表示中的累积压缩会引发“过压缩”问题\cite{Alon2020OnTB}，远距离结构信号在传播过程中易被淹没，在大规模稀疏图上尤为突出。
为缓解这些问题，研究者提出了多种正则化与结构改进策略，如 MixHop\cite{AbuElHaija2019MixHopHG} 通过显式聚合多阶邻域特征提升多尺度表达能力，
Jump Knowledge 网络\cite{Xu2018RepresentationLO}通过跨层跳连与层级特征聚合缓解深层表征退化，
DropEdge\cite{Rong2019DropEdgeTD}通过随机丢弃部分边改善过平滑并提升训练稳定性，相关“tricks”在系统性基准研究中得到了总结\cite{Wang2021BagOT}。
然而，这些方法大多仍在消息传递与正则化层面进行局部改进，对超大规模图的可扩展性、对非结构模态（文本、图像等）的直接建模能力以及与通用大模型的无缝集成仍然存在明显不足\cite{Wang2023CanLM}，
这也为后续基于大规模语言模型的图结构计算范式与 Graph-LLMs 的发展留下了广阔空间。

\subsection{基于 Transformer 的方法}

近年来，随着 Transformer \cite{Vaswani2017AttentionIA}在自然语言处理\cite{Lin2021ASO}与计算机视觉中\cite{Han2020ASO}的成功应用，
研究者开始系统性地探索如何将基于注意力（Attention）的架构扩展到图结构数据，
形成所谓的图 Transformer（Graph Transformers,GAs）\cite{Ying2021DoTR}方法族 。
这一方向的核心目标是在保留 Transformer 全局建模与强大表达能力的同时，引入必要的图结构归纳偏置，
以缓解传统 GNN 在长程依赖建模与表达能力上限方面的固有局限。

在模型设计上，第一类代表性工作侧重于在标准 Transformer\cite{Vaswani2017AttentionIA} 中显式注入图结构与位置编码。
Dwivedi 和 Bresson 提出的 Graph Transformer（GT） \cite{Dwivedi2020AGO}将图节点视作 token，
并通过图拉普拉斯特征、最短路径距离等构造谱域或几何位置编码，使全局自注意力在计算时仍对拓扑邻接关系保持敏感；
Mialon 等人\cite{Mialon2021GraphiTEG}的 GraphiT 则系统分析了“无结构编码时 Transformer 退化为节点多重集合（bag-of-nodes）模型”的问题，
提出将结构编码嵌入注意力权重和 token 表示中，以避免结构信息被完全丢失\ 。
然而，处理大规模图的计算量很大，全局注意力机制无法有效地捕获图的拓扑结构。为了缓解这个问题，一些方法将图结构信息合并到注意力矩阵中。
Ying 等人\cite{Ying2021DoTR}提出的 Graphormer 结合节点中心性编码、最短路径距离编码和边特征编码，
在不改变标准 Transformer 主干的前提下，通过结构偏置矩阵显式建模全局拓扑。
Chen 等人\cite{Chen2022StructureAwareTF}的 Structure-Aware Transformer（SAT）则以“子图感知”为核心，先为每个节点提取根节点子图特征，再在此基础上计算注意力权重，
从理论与实验上证明该结构感知注意力在区分结构相似性方面优于仅依赖位置编码的方案\ 。

与上述依赖精心设计结构偏置的路线相比，第二类工作尝试尽量减少图特定设计，直接用“纯 Transformer”学习图。
Kim 等人\cite{Kim2022PureTA}提出的 Tokenized Graph Transformer（TokenGT）以及
Chen 等人\cite{Chen2022NAGphormerAT}的NAGphormer将节点与边统一视作独立 token，
通过正交随机特征或拉普拉斯特征构造节点标识向量，并结合可学习的类型嵌入直接输入标准 Transformer，
在架构上完全摒弃显式消息传递与手工结构偏置。理论分析表明，这类方法的表达能力至少不弱于一类二阶等变图网络（2-IGN）\cite{Han2022GeometricallyEG}，
在大规模分子数据集 PCQM4Mv2 \cite{Hu2021OGBLSCAL} 上显著超越主流 GNN 基线，并与多种精心设计结构偏置的图 Transformer 表现相当\cite{Chen2022NAGphormerAT}。
随后的一系列工作进一步检验了“纯注意力架构”的上限，例如 Buterez 等人\cite{Buterez2024AnEA}提出的端到端注意力图模型，
将图视作边集合，通过简单的编码器与注意力池化，在无需复杂预处理与额外模块的前提下，
即可在多类图基准上获得可与调优 GNN 及图 Transformer 竞争的性能；
Ma 等人\cite{Ma2025SimplifyingGT}提出的 Simplifying Graph Transformers 则在保持主干接近标准 Transformer 的前提下，
仅通过相对位置偏置、轻量门控与规范化策略，即显著提升了对复杂图结构的判别能力与长程依赖建模能力。
这一脉络的结果表明，在配合合适的标识编码、结构感知位置编码与正则化策略后，纯 Transformer 架构本身即具备较强的图学习潜力，
为构建与语言/视觉基础模型更易对齐的统一架构提供了可行路径。

第三类方法则更强调领域特化与 GNN–Transformer 的融合，尤其在分子与材料科学等应用场景中发展迅速。
Rong 等人\cite{Rong2020GROVERSM}提出的 GROVER 将消息传递网络嵌入 Transformer 框架，通过节点级、边级与图级多粒度自监督任务，在千万级无标注分子图上预训练得到通用分子表示，在多种分子性质预测基准上显著优于传统 GNN。
Maziarka 等人\cite{Maziarka2020MoleculeAT}的 Molecule Attention Transformer（MAT） 通过将原子间欧氏距离、键连接关系显式注入自注意力权重，在不改变总体架构简洁性的前提下，系统性提升了跨多类分子性质任务的性能与稳定性。
Anselmi 等人\cite{Anselmi2024MolecularGT} 提出的 Molecular Graph Transformer（MGT）则在原始键图及其线图上联合进行局部注意力与消息传递，有效捕获分子中的长程相互作用，在量子化学性质预测任务上超过 ALIGNN 等专用 GNN 架构。
与此同时，Rampášek 等人\cite{Rampek2022RecipeFA} 提出的 GraphGPS与 Masters 等人\cite{Masters2022GPSAO}提出的 GPS++ 给出了一种混合式“MPNN + Transformer”通用配方：在局部采用高效消息传递捕获近邻结构，在全局采用稀疏或线性复杂度注意力实现长程依赖建模，并通过 2D/3D 结构编码与辅助去噪任务在 PCQM4Mv2 \cite{Hu2021OGBLSCAL} 等大规模分子基准上取得领先性能。

综上，基于 Transformer 的图方法显著提升了图表示学习在长程依赖建模与任务性能上的上限。
然而，无论是通过在注意力矩阵中注入复杂结构偏置，还是通过 token 化节点与边、叠加多种位置/类型编码，
现有图 Transformer 往往以间接方式表达图拓扑，计算代价与模型复杂度均较高；
同时，多数方法仍主要面向结构单模态，尚难直接支撑统一的图–语言基础建模框架。

\subsection{基于大语言模型的方法}



\section{论文研究内容}

此部分必须详细描述，
必要时可划设小节。
国外学位论文的Introduction章基本仅阐述此内容。
为研究开展的相关工作和实验，
此间遇到何难处及对应的解法。
对论文研究领域不甚了解的评阅老师，
希望从摘要和此小节尽可能的了解最多信息。


\section{论文组织结构}

简明扼要的介绍下各章主旨，版面控制半页内。
