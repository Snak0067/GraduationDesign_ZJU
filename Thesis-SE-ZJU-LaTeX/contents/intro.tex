% !TEX root = ../main.tex

% 第一章一般名为绪论／引言，不可省略

\chapter{绪论}

\section{课题背景与研究意义}

在当前人工智能技术广泛赋能的时代，结构化图数据\cite{图结构数据1,图结构数据2}在多个关键领域中扮演着基础性角色。
社交网络中用户间的关注、互动构成异质社交图，用于社交网络分析\cite{社交网络分析1,社交网络分析2}与推荐系统 \cite{He2020LightGCNSA,Wang2021LearningIB}；
生物医药领域中分子结构与蛋白质交互天然以图形式建模，广泛用于生物网络分析\cite{Guo2022GraphbasedMR,Liu_Wang_Vu_Moretti_Bodenheimer_Meiler_Derr_2023}与分子生成\cite{Hu2020OpenGB}；
交通调度与城市计算将路网与交通流抽象为图模型，用于路径优化与实时控制\cite{Zheng2019GMANAG}；
金融风控利用交易网络进行欺诈检测和信用建模\cite{eFraudCom}；
电子电路与EDA设计中电路拓扑本身即为图结构\cite{DeepGate}。
随着多领域数据规模与复杂性不断增长，图结构在表示复杂实体间多关系、高阶依赖方面展现出不可替代的表达能力。
因此，如何从图结构中高效抽取语义、理解拓扑规律，并迁移到各类图计算任务中，成为当前人工智能研究中的重要课题之一。

传统的图结构计算
主要依赖符号方法\cite{Blondel2008FastUO,随机游走,谱聚类,标签传播,图同构检测}与
统计学习方法\cite{Perozzi2014DeepWalkOL,Grover2016node2vecSF,Tang2015LINELI,高阶邻接嵌入,有向图高阶传递性嵌入}。
符号方法以图论与谱分析为核心，
通过模块度优化\cite{Blondel2008FastUO}、随机游走\cite{随机游走}、谱聚类\cite{谱聚类}、标签传播\cite{标签传播}及图同构检测\cite{图同构检测}等手段揭示图的拓扑规律，具有良好的可解释性，
但在处理大规模、动态与异质图时受限于计算复杂度与人工特征设计。
统计学习方法则通过随机游走\cite{Perozzi2014DeepWalkOL,Grover2016node2vecSF}与矩阵分解\cite{Tang2015LINELI,高阶邻接嵌入,有向图高阶传递性嵌入}等嵌入技术，
将离散图结构映射至连续向量空间，以实现节点、边及子图的低维表征。
然而，这些传统范式普遍存在局部表达受限，在大规模图计算中面临效率与泛化性挑战。

随着图神经网络（GNNs）的发展，图学习领域逐步从传统的机器学习转向深度学习。
然而，过平滑现象（over-smoothing）\cite{Rusch2023ASO}和过压缩问题（over-squashing）\cite{Alon2020OnTB}问题共同限制了 GNNs 在大规模图数据上的表达能力与可扩展性。
为突破这一限制，研究者开始探索以大规模语言模型（如BERT\cite{BERT}, T5\cite{T5}, LLaMA\cite{Llama}）为核心的图结构计算新范式。
该方向旨在将图的结构关系与语义信息统一映射至语言空间，使模型具备跨节点、跨关系的全局推理能力。
依托语言模型卓越的语义理解与生成式推理机制，图数据获得了语义补全、自解释与可迁移的潜能，展现出超越传统图神经网络的泛化能力与知识整合能力。
在这一趋势下，图大语言模型（Graph Large Language Models,Graph-LLMs） 逐渐成为连接符号推理与语义建模的关键桥梁。它们试图让语言模型具备理解、推理并生成图结构信息的能力，从而在统一的语义框架中实现图计算与自然语言处理的融合。

尽管 Graph-LLMs 在图语义理解与推理方面取得初步进展，现有研究仍面临一些技术性挑战和局限性，主要集中在以下几个方面：

\textbf{1）图结构编码存在信息缺失与拓扑保真不足：} 现有做法多依赖邻域采样与简化线性化，将图转换为可读序列或片段式子图。此过程易引入起点与遍历路径偏置，边与连通性被截断或重排，环、桥接与长链路等关键结构难以被完整呈现。随之而来的，是对路径可达性、中心性与全局属性的估计偏差，导致在节点分类、链接预测与结构问答等任务上出现系统性性能下滑。

\textbf{2）表示保真与跨模态对齐不足：} 将图转写为文本后，结构信号被弱化为局部片段，缺乏对多阶邻居与全局依赖的统一刻画；同时，语言模型天然缺少图归纳偏置，图与文本嵌入空间难以实现稳定的一一对应。结果表现为：全局与局部语义映射不一致，节点/边级细粒度语义难以锚定到相应词片，跨样本与跨域的表示漂移加剧，模型对复杂拓扑的理解与迁移显著受限。

\textbf{3）推理建模与泛化能力不足：} 以监督微调为主的训练范式往往学习任务捷径而非通用结构规律，模型多给出“结论式”回答而缺少可核验的中间推理链，导致可解释性与稳健性不足。面对未见任务、异构图或尺度变化时，表现出明显的域外退化：对长距离依赖的判断不稳定，对结构扰动与数据漂移敏感，零样本与小样本条件下的迁移能力有限。

尽管存在上述挑战，近年来深度学习技术的演进，尤其是跨模态表征学习与生成式预训练范式的发展，为图大语言模型的进一步突破奠定了基础。
一方面，多模态融合模型在视觉、语音、文本等领域的成功实践表明，大模型具备强大的跨域语义抽象与表示迁移能力，为图模态与语言模态的统一建模提供了新的范式支撑。
另一方面，图结构序列化编码、对比学习与指令微调等方法的引入，使语言模型能够在结构感知的前提下学习图的拓扑依赖与语义关联，提升模型在复杂图任务中的理解与推理能力。

\section{相关研究工作}

对于国内硕士学位论文来说，
一般较少研究完全无前人探索的领域，
所以有必要交待前人在此做出的努力和尝试。
同样，请提供数据和引用保证严谨。

为避免引起评阅老师判定有凑篇幅之嫌，
请有针对的描述前人研究的不足之处，
做到``有破有立''。
\subsection{传统图表示学习}
\subsubsection{图神经网络}

长期以来，图神经网络（Graph Neural Networks, GNN）\cite{Kipf2016SemiSupervisedCW,Wu2019ACS,Han2023PiVePW,Wu2019SimplifyingGC,Wang2021BagOT}
一直是图机器学习的主导范式。GNN 通过消息传递与邻域聚合机制，在统一框架下同时建模节点特征与图拓扑结构。从模型设计范式来看，基于不同的消息传递与卷积定义方式，主流 GNN 模型大致可以分为谱域方法和空间域方法两大类。


谱域方法以图拉普拉斯谱理论为基础，在频域中定义卷积核再映射回节点空间。Defferrard 等人\cite{Defferrard2016ConvolutionalNN}提出的 ChebNet
通过切比雪夫多项式对谱卷积进行局部近似，显著降低了谱滤波的计算成本；
Kipf 和 Welling等人\cite{Kipf2016SemiSupervisedCW} 提出的图卷积网络（GCN）
在 ChebNet 的基础上进一步采用一阶谱近似，形成简单而高效的“归一化邻接矩阵 + 线性变换”结构，被广泛视为谱域 GNN 的里程碑工作；
随后，Wu 等人\cite{Wu2019SimplifyingGC}从信号平滑视角对 GCN 进行简化，揭示了多层 GCN 本质上是一种低通滤波器，为后续关于过平滑现象的研究奠定了理论基础。

与之相对，空间域方法直接在图的邻接结构上定义局部消息传递规则，通过聚合节点一阶或多阶邻域特征完成表示更新。
Hamilton 等人\cite{Hamilton2017InductiveRL}提出的 GraphSAGE
针对 GCN 难以处理未见节点的问题，构建了“邻域采样 + 特征聚合”的归纳式框架，支持在大规模动态图上进行在线推断；
Veličković 等人\cite{Velickovic2017GraphAN}提出的图注意力网络GAT
将自注意力机制引入邻域聚合，通过学习可变权重区分不同邻居的重要性，缓解了简单平均聚合对噪声节点与不相关邻居的过度平滑问题；
在此基础上，Brody 等人\cite{Brody2021HowAA}指出原始 GAT 的注意力打分实质上是“静态”的，难以捕捉特征变换后的高阶依赖关系，并提出动态注意力变体以提升表达灵活性。
Chen 等人\cite{Chen2021EdgeFeaturedGA}进一步沿注意力方向扩展，提出显式建模边特征的 Edge-featured GAT（EGAT），
通过对节点和边分别构建注意力通道、在多层结构中进行多尺度融合，使 GNN 能够在节点与边两个层面同时学习表征。

在提升表达能力方面，Xu 等人\cite{Xu2018HowPA}提出的GIN
通过在聚合时显式引入自环并采用求和聚合，在理论上证明其判别能力与 Weisfeiler–Lehman 图同构测试等价，显著增强了模型对局部子结构模式的区分能力。
Monti 等人\cite{Monti2016GeometricDL}则从几何深度学习视角出发，将图卷积推广至一般流形与不规则网格，通过构造局部坐标系与核函数，在统一框架下处理社交网络、交通网络等空间图数据。

尽管取得了大量进展，GNN 也暴露出一系列固有局限。
一方面，随着网络层数加深，节点表征逐渐趋于同质化，导致不同类别或结构位置的节点难以区分，即“过平滑”现象\cite{Rusch2023ASO}；
另一方面，多跳依赖在固定维度表示中的累积压缩会引发“过压缩”问题\cite{Alon2020OnTB}，远距离结构信号在传播过程中易被淹没，在大规模稀疏图上尤为突出。
为缓解这些问题，研究者提出了Jump Knowledge \cite{Xu2018RepresentationLO}、MixHop\cite{AbuElHaija2019MixHopHG}以及DropEdge\cite{Rong2019DropEdgeTD}等多种改进策略，
然而，这些方法大多仍在消息传递与正则化层面进行局部改进，无法直接处理文本或图像等非数字原始数据，需要额外的特征工程技术\cite{Wang2021BagOT}。
同时，GNN与现有大规模生成模型缺乏兼容性，这对于与视觉和语言等其他人工智能领域集成到统一的智能系统中提出了重大挑战。

\subsubsection{图Transformer}

近年来，随着 Transformer \cite{Vaswani2017AttentionIA}在自然语言处理\cite{Lin2021ASO}与计算机视觉中\cite{Han2020ASO}的成功应用，
研究者开始系统性地探索如何将基于注意力（Attention）的架构扩展到图结构数据，
形成所谓的图 Transformer（Graph Transformers,GAs）\cite{Ying2021DoTR}方法族 。
这一方向的核心目标是在保留 Transformer 全局建模与强大表达能力的同时，引入必要的图结构归纳偏置，
以缓解传统 GNN 在长程依赖建模与表达能力上限方面的固有局限。

在模型设计上，第一类代表性工作侧重于在标准 Transformer\cite{Vaswani2017AttentionIA} 中显式注入图结构与位置编码。
Dwivedi 和 Bresson 提出的 Graph Transformer（GT） \cite{Dwivedi2020AGO}将图节点视作 token，
并通过图拉普拉斯特征、最短路径距离等构造谱域或几何位置编码，使全局自注意力在计算时仍对拓扑邻接关系保持敏感；
Mialon 等人\cite{Mialon2021GraphiTEG}的 GraphiT 则提出将结构编码嵌入注意力权重和 token 表示中，以避免结构信息被完全丢失\ 。
然而，处理大规模图的计算量很大，全局注意力机制无法有效地捕获图的拓扑结构。为了缓解这个问题，一些方法将图结构信息合并到注意力矩阵中。
Ying 等人\cite{Ying2021DoTR}提出的 Graphormer 结合节点中心性编码、最短路径距离编码和边特征编码，
在不改变标准 Transformer 主干的前提下，通过结构偏置矩阵显式建模全局拓扑。
Chen 等人\cite{Chen2022StructureAwareTF}的 SAT则以“子图感知”为核心，先为每个节点提取根节点子图特征，再在此基础上计算注意力权重，
理论与实验证明该结构感知注意力在区分结构相似性方面优于仅依赖位置编码的方案\ 。

与上述依赖精心设计结构偏置的路线相比，第二类工作尝试尽量减少图特定设计，直接用“纯 Transformer”学习图。
Kim 等人\cite{Kim2022PureTA}提出的 Tokenized Graph Transformer（TokenGT）以及
Chen 等人\cite{Chen2022NAGphormerAT}的NAGphormer将节点与边统一视作独立 token，
通过正交随机特征或拉普拉斯特征构造节点标识向量，并结合可学习的类型嵌入直接输入标准 Transformer，
在架构上完全摒弃显式消息传递与手工结构偏置。理论分析表明，这类方法的表达能力至少不弱于一类二阶等变图网络（2-IGN）\cite{Han2022GeometricallyEG}，
在大规模分子数据集 PCQM4Mv2 \cite{Hu2021OGBLSCAL} 上显著超越主流 GNN 基线，并与多种精心设计结构偏置的图 Transformer 表现相当\cite{Chen2022NAGphormerAT}。
随后的一系列工作进一步检验了“纯注意力架构”的上限，例如 Buterez 等人\cite{Buterez2024AnEA}提出的端到端注意力图模型，
将图视作边集合，通过简单的编码器与注意力池化，在无需复杂预处理与额外模块的前提下，
即可在多类图基准上获得可与调优 GNN 及图 Transformer 竞争的性能；
Ma 等人\cite{Ma2025SimplifyingGT}提出的 Simplifying Graph Transformers 则在保持主干接近标准 Transformer 的前提下，
仅通过相对位置偏置、轻量门控与规范化策略，即显著提升了对复杂图结构的判别能力与长程依赖建模能力。

从整体上看，基于 Transformer 的图方法已经在表达能力与任务性能上展现出超越传统 GNN 的潜力 。
然而，现有图 Transformer 仍面临若干开放问题：一是全局自注意力在大图上的二次复杂度限制了其可扩展性，
实际应用中往往需要借助局部注意力或稀疏化策略在表达力与效率之间折中；
二是结构与位置编码的设计高度依赖任务与图分布，不同工作采用的偏置方式差异较大，
缺乏统一的理论框架与稳健的跨任务迁移能力；三是大多数方法仍主要针对单一模态图数据，
如何在统一架构下同时处理图结构与自然语言、视觉等模态，使图 Transformer 成为真正的图多模态基础模型，仍是当前研究的前沿方向。

\subsection{图基础模型}
受到基础模型（Foundation Models）在 自然语言处理\cite{Lin2021ASO}与计算机视觉中\cite{Han2020ASO}方面的成功的启发，
图学习最近进入了图基础模型（Graph Foundation Models,GFMs）\cite{wangGraphFoundationModels2025}时代。
这些模型\cite{Wang2024GFTGF,Shoghi2023FromMT,Kong2024GOFAAG}使用自监督目标\cite{Liu2021GraphSL}在大规模图上进行了预训练，
使它们能够学习跨任务和跨域转移的通用表示。
GFMs整合了结构依赖性和语义内容，表现出很强的零样本和少样本泛化。通过将模型训练与特定任务解耦， GFMs减少了对标记数据和特定领域启发式方法的依赖，使该领域更接近通用图智能。
% htbp 什么的现在不要管
\begin{figure}[htbp]
    \centering  % 学位论文规定图表皆水平居中于版心 在 zjuthesis.cls 搜「版心设置」
    \includegraphics[width = .8\linewidth]{1_Intro/TheWorkflowOfGraphFoundationModels.png} % 设定图片宽度相对于版心宽度，图片文件资源名
    \caption{图基础模型的工作流程\cite{wangGraphFoundationModels2025}} % 图的题注
    \label{fig:TheWorkflowOfGraphFoundationModels} % 与 autoref 关联，设定交叉引用和显示「图x.x」
\end{figure}

\subsubsection{基于图神经网络的图基础模型}

以图神经网络（GNN）为主干的架构凭借其对非欧氏空间的原生建模能力，构成了当前最成熟的范式。
此类模型通过大规模自监督预训练后轻量微调的范式，平衡结构归纳偏置与跨任务泛化能力，为图表示学习提供统一解决方案。

从预训练策略维度，现有工作可系统划分为对比式与生成式两类。
在对比式预训练方向，Velickovic等人\cite{Velickovic2018DeepGI}首次将 InfoMax 思想系统引入无监督图表示学习，
以节点级表示与全图摘要向量的互信息最大化为目标，通过局部-全局表示对齐捕获结构信息，但其适用场景局限于单图，跨图迁移能力较弱。
Qiu等人\cite{Qiu2020GCCGC} 进一步将对比学习扩展至跨网络场景，通过在大规模图集合上构造锚点子图并开展实例级对比，
显著提升模型结构感知能力，却对采样策略与结构视角的选择高度敏感。
You等人\cite{You2020GraphCL} 系统化探究了图结构增强算子组合，确立了多视图对比学习标准流程，有效提升图级任务精度。
Zhu等人\cite{Zhu2020DeepGC} 则面向节点级表示学习，通过特征扰动与结构扰动构造双视图进行对比，缓解了标注稀缺问题，然而在处理超大规模图与异构结构时仍面临扩展性瓶颈。


在生成式预训练方向，研究更侧重对图结构与属性的显式建模。
Kipf等人\cite{Kipf2016VariationalGA} 将变分自编码器（VAE）\cite{Lopez2020DecisionMakingWA}迁移至图域，
通过重构邻接矩阵学习全局结构分布，是最早被系统提出的生成式图自监督框架之一，但对高维节点属性与复杂拓扑的刻画能力有限。
Hu等人\cite{Hu2020GPTGNNGP} 在此基础上提出多任务预训练框架，同步优化边生成与属性预测目标，显著提升异构图建模能力。
GraphMAE系列\cite{Hou2022GraphMAESM} 通过 “掩码节点特征重建” 替代直接重构邻接矩阵，降低对精确结构重建的依赖，提升大规模图上的训练稳定性与效率，
然而其结构建模仍较间接，对高阶拓扑模式（如环、模体）的表达能力有限
。面向分子图的 GROVER\cite{Rong2020GROVERSM} 则结合多粒度子图上下文预测与化学模体（motif）预测，将领域先验融入图 Transformer 架构，
在分子性质预测任务中取得显著优势。

在下游适配方式上，图提示学习（graph prompting）已成为提升 GNN 基础模型复用效率的关键方向。
Liu等人\cite{Liu2023GraphPromptUP} 通过在输入特征层或隐藏层注入可学习提示向量，将任务信息与结构表示绑定，冻结大部分 GNN 参数即可适配多任务，
大幅降低微调成本。Yu等人\cite{Yu2024NonHomophilicGP} 提出基于原型（prototype）的图提示策略，通过在表示空间学习任务原型指导 GNN 适配，在少样本场景中性能提升显著。
针对异质图与时序图，CPT-HG\cite{Jiang2021ContrastivePO}、MultiGPrompt\cite{Yu2023MultiGPromptFM} 
等工作通过在 GNN 骨干与提示机制中显式编码类型、时间等元信息，扩展了 GFMs 的图类型覆盖范围。

以图神经网络（GNN）为主干的 GFMs 充分发挥了消息传递的局部归纳偏置与参数共享特性，在标签稀缺、任务多样的图学习场景中展现出良好的可扩展性。
但该技术路线仍有局限性：一方面，其对节点或边携带的文本等丰富语义信息利用不足，
难以整合外部知识；另一方面，模型表征空间与大规模语言模型（LLMs）缺乏显式对齐机制，当任务需同时依赖复杂拓扑关系与高层语义时，其表达能力仍存在瓶颈。

\subsubsection{基于大语言模型的图基础模型}

近年来，一类以大语言模型作为主干、通过图–语言协同训练获得统一图表征的 Graph-LLMs 逐渐形成。
典型做法是先将图结构转换为文本或结构化提示，再以 LLM 作为唯一的推理与决策核心，通过指令微调或自监督对齐获得跨任务的图推理能力。

Lin等人\cite{Lin2024LangGFMAL}开创性地提出自监督对齐策略，通过拓扑自编码与特征掩码自编码两大预训练目标，使LLMs习得图结构语义。
该方法无需显式图操作即可重构图拓扑属性，有效缓解了标注稀缺问题，但在超大规模图上面临计算效率挑战。
Yu等人\cite{Yang2024GraphAgentAG}引入层次化上下文编码机制，通过多跳邻居信息的递归整合增强结构感知能力；其智能体决策框架动态调整系统提示，显著提升大型图的推理效率。
Zhang等人\cite{Zhang2023GraphToolFormerTE}采用API增强的推理范式，通过外部工具调用实现动态图遍历与数据检索，尤其适用于知识图谱补全等复杂任务，
但需要精心设计工具接口与调用逻辑。

在指令微调方向，Wang等人\cite{Wang2024InstructGraphBL} 将图推理任务重构为指令遵循问题，结合偏好对齐技术（如RLHF）优化输出忠实度。
其多提示训练策略统一了分类、生成与摘要等异构任务，但在复杂拓扑模式识别方面仍显不足。
Ye等人\cite{Ye2023LanguageIA} 进一步通过多任务指令统一不同图数据集的输入-输出格式，实现了跨架构适应性，
但高度依赖大规模人工构造的指令数据，微调成本较高，且模型的图结构感知仍主要来自模板化的图线性化过程。

上下文学习（In-Context Learning,ICL）机制在零样本场景展现出巨大潜力。GraphICL\cite{Sun2025GraphICLUG}构建结构化提示框架，
通过任务描述、锚点文本、拓扑上下文与标注示例四元组件，使通用LLM在低资源与域外任务中超越专用图模型。
Li等人\cite{Li2025AreLL}显式整合查询节点与邻居上下文，将图结构嵌入提示作为内在特征源，大幅提升关系依赖建模能力。然而，示范选择质量对性能影响显著，且在节点依赖复杂的图中构造高质量示范仍具挑战性。

总体来看，以 LLM 为主干的图基础模型取得了重要进展，初步展示了单一 LLM 统一多图任务、多域图数据的潜力。
然而，它们普遍依赖图到文本的序列化过程，图结构在进入 LLM 之前已被压缩或重排；缺乏针对多层表示的细粒度结构约束，
使得长程依赖、环结构与桥接模式等关键拓扑在语义空间中的保真刻画仍不充分。这些局限也正为后续工作提供了改进空间。

\subsection{图结构-语义文本跨模态对齐}

图结构与语义文本的跨模态对齐是 Graph Foundation Models（GFMs）突破单模态局限的核心环节。
图神经网络（GNN）擅长捕捉拓扑依赖但语义建模薄弱，大型语言模型（LLM）具备强大文本理解能力却缺乏结构归纳偏置，
而对比预训练（Contrastive Pretraining）通过构建模态间正负样本对、优化表示空间的区分性，成为弥合二者鸿沟的关键技术路径 。

Zhu等人提出的GraphCLIP\cite{Zhu2024GraphCLIPET}借鉴视觉-语言预训练（Visual-Language Pre-Training, VLP）模式，
设计双编码器架构：GNN 编码器（如 GraphSAGE\cite{Hamilton2017InductiveRL}）提取图结构表示，LLM 编码器（如 SentenceBERT\cite{Reimers2019SentenceBERTSE}）编码节点或图的文本描述，通过 InfoNCE 损失最大化同一样本（图 - 文本对）的模态相似度、最小化异样本相似度。
该方法在零样本图检索、跨域分子性质预测任务中验证了对齐有效性。
而Brannon等人\cite{Brannon2023ConGraTSC} 提出跨模态监督增强的对比预训练策略：不再依赖人工图 - 文本对，
而是通过 LLM 生成伪文本描述，同时在对比损失中融入结构约束 —— 将 GNN 输出的邻域相似度作为权重，引导 LLM 文本表示向结构相关方向更新。
该工作解决了 GraphCLIP 对标注数据的强依赖问题，通过伪标签生成扩展了训练数据规模，且通过结构约束增强了文本表示的拓扑相关性；
Lin等人\cite{Lin2024GT2VecLL}创新性地将LLM作为多模态编码器处理图结构化数据，通过对比学习拉近语义相似节点的嵌入距离，在低资源场景下超越专用图模型。
Zhao等人\cite{Zhao2022LearningOL} 进一步突破静态对比框架的局限，采用变分 EM 式训练过程，在 E 步中利用 GNN 进行结构推断，在 M 步中用 LLM 更新文本表征，并通过对比 / 辅助损失将两者反复对齐，逐步逼近结构–语义一致的节点嵌入。
Sun等人\cite{Sun2023LargeLM} 则反向利用 LLM 评估节点对语义相似度，对图拓扑进行增删边修正，再结合伪标签传播提升分类与聚类性能。 

现有基于对比预训练的图–文跨模态对齐方法，
从双任务预训练到双编码器对比与EM混合优化，逐步强化了图结构与语义文本之间的一致性，
但整体仍存在三个共性局限：其一，对齐粒度偏粗，缺乏从节点 / 边到句子 / 词片的多层次约束；
其二，图结构多在 GNN 中被先行“折叠”，进入 LLM 后仅以若干致密向量形式存在，结构保真与可逆性有限；
其三，预训练目标多围绕文档图等特定场景设计，对开放域图任务和更复杂推理需求的泛化能力仍有提升空间。
这一系列不足，直接构成了本文从结构保真编码与多层对齐损失两个维度重新设计 Graph-LLM 训练范式的出发点。

\subsection{强化学习驱动的图结构推理范式}

强化学习（Reinforcement Learning, RL）为图结构理解与推理提供了突破传统监督学习范式的重要途径。
与依赖静态标注数据的图神经网络（GNN）相比，RL通过智能体与环境的交互及奖励信号优化策略，
使模型能够在复杂拓扑空间中主动探索、反复试错，从而学习面向目标任务的结构感知决策规则。
这一特性为图基础模型（Graph Foundation Models, GFMs）在跨任务、跨域场景下构建可迁移的图推理能力奠定了方法论基础。  

早期研究主要将RL应用于图上的组合优化任务。Bello等人\cite{Bello2016NeuralCO}提出的神经组合优化（Neural Combinatorial Optimization, NCO）框架，将旅行商问题等经典任务建模为序列决策过程，结合指针网络与策略梯度方法，实现了无需人工启发式的图结构求解策略学习。Dai等人\cite{Khalil2017LearningCO}进一步将GNN集成至策略网络，通过图结构编码和策略梯度优化，在最大割、最小顶点覆盖等任务上显著提升了求解质量与泛化性能。这些工作虽验证了RL在图决策中的有效性，但其任务特定性限制了跨领域迁移能力。

在结构生成与工程优化领域，RL展现出引导复杂约束下策略学习的独特优势。You等人\cite{You2018GraphCP}开发的图卷积策略网络（GCPN）将分子建模为图结构，以分子性质评分为奖励，通过图卷积策略网络逐步生成满足化学合法性与功能优化的分子结构。Mirhoseini等人\cite{Mirhoseini2021AGP}将芯片版图设计转化为图上顺序决策问题，利用图网络构建策略-价值函数，在功耗与时延指标上超越了人工设计的启发式方法。Li等人\cite{Li2025CanOD}和Wang等人\cite{Wang2020GCNRLCD}分别通过数据策略迁移和跨工艺节点优化，显著提升了多领域图推理的泛化能力与工程实用性。

近期进展将RL扩展至大语言模型驱动的图推理框架。尽管RL已在数学与代码推理领域证明其价值（如OpenAI o1\cite{ElKishky2024OpenAIOS}、DeepSeek R1\cite{DeepSeekAI2025DeepSeekR1IR}），但其在图推理中的可扩展性长期缺乏系统验证。Guo等人\cite{Guo2025G1TL}提出的G1框架构建了可自动判分的图环境（Erdős基准），通过可验证奖励对LLM进行RL微调，在连通性判定、最短路径、哈密顿路径等任务上显著提升了推理准确率，证实了从规则可验证奖励中学习图算法的可行性。

总体来看，RL-图方法已从早期任务定制型框架逐步演进为支持多任务、多领域的通用推理范式。尽管在分子设计、电路布局和通用图论推理等场景取得了显著进展。
尽管在分子设计、电路优化和图论推理等领域取得重要进展，该领域仍面临样本效率低下、奖励设计依赖领域知识、大规模异构图扩展性不足等核心挑战。
这也为后续结合 Graph-LLMs 与可验证奖励机制，构建更稳健、更具泛化能力的图推理模型留下了重要空间。






\section{论文研究内容}

此部分必须详细描述，
必要时可划设小节。
国外学位论文的Introduction章基本仅阐述此内容。
为研究开展的相关工作和实验，
此间遇到何难处及对应的解法。
对论文研究领域不甚了解的评阅老师，
希望从摘要和此小节尽可能的了解最多信息。


\section{论文组织结构}

简明扼要的介绍下各章主旨，版面控制半页内。
