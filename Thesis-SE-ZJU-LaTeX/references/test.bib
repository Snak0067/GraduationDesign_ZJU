@online{yinwang2013,
    author={王垠},
    year={2013},
    title={谈Linux, Windows 和 Mac},
    language={zh},
    url={http://www.yinwang.org/blog-cn/2013/03/07/linux-windows-mac}
}

@inproceedings{图结构数据1,
author = {Dong, Yushun and Liu, Ninghao and Jalaian, Brian and Li, Jundong},
title = {EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512173},
doi = {10.1145/3485447.3512173},
abstract = {Graph Neural Networks (GNNs) have shown superior performance in analyzing attributed networks in various web-based applications such as social recommendation and web search. Nevertheless, in high-stake decision-making scenarios such as online fraud detection, there is an increasing societal concern that GNNs could make discriminatory decisions towards certain demographic groups. Despite recent explorations on fair GNNs, these works are tailored for a specific GNN model. However, myriads of GNN variants have been proposed for different applications, and it is costly to fine-tune existing debiasing algorithms for each specific GNN architecture. Different from existing works that debias GNN models, we aim to debias the input attributed network to achieve fairer GNNs through feeding GNNs with less biased data. Specifically, we propose novel definitions and metrics to measure the bias in an attributed network, which leads to the optimization objective to mitigate bias. We then develop a framework EDITS to mitigate the bias in attributed networks while maintaining the performance of GNNs in downstream tasks. EDITS works in a model-agnostic manner, i.e., it is independent of any specific GNN. Experiments demonstrate the validity of the proposed bias metrics and the superiority of EDITS on both bias mitigation and utility maintenance. Open-source implementation: https://github.com/yushundong/EDITS.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1259–1269},
numpages = {11},
keywords = {algorithmic fairness, data bias, graph neural networks},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}
@article{图结构数据2,
  title={Meta-Weight Graph Neural Network: Push the Limits Beyond Global Homophily},
  author={Xiaojun Ma and Qin Chen and Yuanyi Ren and Guojie Song and Liang Wang},
  journal={Proceedings of the ACM Web Conference 2022},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247594750}
}
@inproceedings{Han2023PiVePW,
  title={PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs},
  author={Jiuzhou Han and Nigel Collier and Wray L. Buntine and Ehsan Shareghi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258832958}
}
@article{Wang2024LLMsAZ,
  title={LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings},
  author={Duo Wang and Yuan Zuo and Fengzhi Li and Junjie Wu},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.14512},
  url={https://api.semanticscholar.org/CorpusID:271963116}
}
@article{Luo2024GraphconstrainedRF,
  title={Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models},
  author={Linhao Luo and Zicheng Zhao and Chen Gong and Gholamreza Haffari and Shirui Pan},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.13080},
  url={https://api.semanticscholar.org/CorpusID:273403682}
}
@article{Bo2025QuantizingTG,
  title={Quantizing Text-attributed Graphs for Semantic-Structural Integration},
  author={Jianyuan Bo and Hao Wu and Yuan Fang},
  journal={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:280324214}
}
@inproceedings{Xia2024OpenGraphTO,
  title={OpenGraph: Towards Open Graph Foundation Models},
  author={Lianghao Xia and Ben Kao and Chao Huang},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268230376}
}
@article{Yang2024LATEXGCLLL,
  title={LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning},
  author={Haoran Yang and Xiangyu Zhao and Sirui Huang and Qing Li and Guandong Xu},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.01145},
  url={https://api.semanticscholar.org/CorpusID:272367262}
}
@article{Zhu2025LLMAG,
  title={LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models},
  author={Xi Zhu and Haochen Xue and Ziwei Zhao and Wujiang Xu and Jingyuan Huang and Minghao Guo and Qifan Wang and Kaixiong Zhou and Yongfeng Zhang},
  journal={ArXiv},
  year={2025},
  volume={abs/2503.03313},
  url={https://api.semanticscholar.org/CorpusID:276782239}
}
@article{Zhao2023GraphTextGR,
  title={GraphText: Graph Reasoning in Text Space},
  author={Jianan Zhao and Le Zhuo and Yikang Shen and Meng Qu and Kai Liu and Michael M. Bronstein and Zhaocheng Zhu and Jian Tang},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01089},
  url={https://api.semanticscholar.org/CorpusID:263605738}
}
@inproceedings{Ye2023LanguageIA,
  title={Language is All a Graph Needs},
  author={Ruosong Ye and Caiqi Zhang and Runhui Wang and Shuyuan Xu and Yongfeng Zhang},
  booktitle={Findings},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:260887732}
}
@article{vicuna2023,
  title={Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
  author={Chiang, Wei-Lin and Roller, Stephen and Dettmers, Tim and Zettlemoyer, Luke and Fedus, William and Smith, Eric M. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year={2023},
  eprint={2306.05685},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@article{Yang2024GLFusionRT,
  title={GL-Fusion: Rethinking the Combination of Graph Neural Network and Large Language model},
  author={Haotong Yang and Xiyuan Wang and Qian Tao and Shuxian Hu and Zhouchen Lin and Muhan Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.06849},
  url={https://api.semanticscholar.org/CorpusID:274609991}
}
@article{Guo2023GPT4GraphCL,
  title={GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking},
  author={Jiayan Guo and Lun Du and Hengyu Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.15066},
  url={https://api.semanticscholar.org/CorpusID:258865990}
}
@article{Wang2024InstructGraphBL,
  title={InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment},
  author={Jianing Wang and Junda Wu and Yupeng Hou and Yao Liu and Ming Gao and Julian McAuley},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.08785},
  url={https://api.semanticscholar.org/CorpusID:267657659}
}
@article{Tang2023GraphGPTGI,
  title={GraphGPT: Graph Instruction Tuning for Large Language Models},
  author={Jiabin Tang and Yuhao Yang and Wei Wei and Lei Shi and Lixin Su and Suqi Cheng and Dawei Yin and Chao Huang},
  journal={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:264405943}
}
@article{Chen2024LLaGALL,
  title={LLaGA: Large Language and Graph Assistant},
  author={Runjin Chen and Tong Zhao and Ajay Kumar Jaiswal and Neil Shah and Zhangyang Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.08170},
  url={https://api.semanticscholar.org/CorpusID:267636663}
}
@article{Xu2018HowPA,
  title={How Powerful are Graph Neural Networks?},
  author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.00826},
  url={https://api.semanticscholar.org/CorpusID:52895589}
}
@article{Hu2020GPTGNNGP,
  title={GPT-GNN: Generative Pre-Training of Graph Neural Networks},
  author={Ziniu Hu and Yuxiao Dong and Kuansan Wang and Kai-Wei Chang and Yizhou Sun},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220250007}
}
@article{Hou2022GraphMAESM,
  title={GraphMAE: Self-Supervised Masked Graph Autoencoders},
  author={Zhenyu Hou and Xiao Liu and Yukuo Cen and Yuxiao Dong and Hongxia Yang and C. Wang and Jie Tang},
  journal={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248987361}
}
@article{You2020GraphCL,
  title={Graph Contrastive Learning with Augmentations},
  author={Yuning You and Tianlong Chen and Yongduo Sui and Ting Chen and Zhangyang Wang and Yang Shen},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.13902},
  url={https://api.semanticscholar.org/CorpusID:225076220}
}
@article{Lopez2020DecisionMakingWA,
  title={Decision-Making with Auto-Encoding Variational Bayes},
  author={Romain Lopez and Pierre Boyeau and Nir Yosef and Michael I. Jordan and Jeffrey Regier},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.07217},
  url={https://api.semanticscholar.org/CorpusID:211146177}
}
@article{Kipf2016VariationalGA,
  title={Variational Graph Auto-Encoders},
  author={Thomas Kipf and Max Welling},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.07308},
  url={https://api.semanticscholar.org/CorpusID:14249137}
}
@article{Zhu2020DeepGC,
  title={Deep Graph Contrastive Representation Learning},
  author={Yanqiao Zhu and Yichen Xu and Feng Yu and Q. Liu and Shu Wu and Liang Wang},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.04131},
  url={https://api.semanticscholar.org/CorpusID:219531264}
}
@article{Velickovic2018DeepGI,
  title={Deep Graph Infomax},
  author={Petar Velickovic and William Fedus and William L. Hamilton and Pietro Lio’ and Yoshua Bengio and R. Devon Hjelm},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.10341},
  url={https://api.semanticscholar.org/CorpusID:52877454}
}
@article{Qiu2020GCCGC,
  title={GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training},
  author={Jiezhong Qiu and Qibin Chen and Yuxiao Dong and Jing Zhang and Hongxia Yang and Ming Ding and Kuansan Wang and Jie Tang},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:219720871}
}
@article{Liu2021GraphSL,
  title={Graph Self-Supervised Learning: A Survey},
  author={Yixin Liu and Shirui Pan and Ming Jin and Chuan Zhou and Feng Xia and Philip S. Yu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  volume={35},
  pages={5879-5900},
  url={https://api.semanticscholar.org/CorpusID:232076112}
}
@article{Kong2024GOFAAG,
  title={GOFA: A Generative One-For-All Model for Joint Graph Language Modeling},
  author={Lecheng Kong and Jiarui Feng and Hao Liu and Chengsong Huang and Jiaxin Huang and Yixin Chen and Muhan Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.09709},
  url={https://api.semanticscholar.org/CorpusID:271212642}
}

@article{Shoghi2023FromMT,
  title={From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction},
  author={Nima Shoghi and Adeesh Kolluru and John R. Kitchin and Zachary W. Ulissi and C. Lawrence Zitnick and Brandon M. Wood},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.16802},
  url={https://api.semanticscholar.org/CorpusID:264451932}
}
@article{Wang2024GFTGF,
  title={GFT: Graph Foundation Model with Transferable Tree Vocabulary},
  author={Zehong Wang and Zheyuan Zhang and Nitesh V. Chawla and Chuxu Zhang and Yanfang Ye},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.06070},
  url={https://api.semanticscholar.org/CorpusID:273962643}
}
@online{wangGraphFoundationModels2025,
  title = {Graph {{Foundation Models}}: {{A Comprehensive Survey}}},
  shorttitle = {Graph {{Foundation Models}}},
  author = {Wang, Zehong and Liu, Zheyuan and Ma, Tianyi and Li, Jiazheng and Zhang, Zheyuan and Fu, Xingbo and Li, Yiyang and Yuan, Zhengqing and Song, Wei and Ma, Yijun and Zeng, Qingkai and Chen, Xiusi and Zhao, Jianan and Li, Jundong and Jiang, Meng and Lio, Pietro and Chawla, Nitesh and Zhang, Chuxu and Ye, Yanfang},
  date = {2025-05-21},
  eprint = {2505.15116},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.15116},
  url = {http://arxiv.org/abs/2505.15116},
  urldate = {2025-09-25},
  abstract = {Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs—characterized by non-Euclidean structures and complex relational semantics—poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, generalpurpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope—universal, task-specific, and domain-specific—and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and generalpurpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  annotation = {TLDR: This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms.},
  file = {C:\Users\Lenovo\Zotero\storage\27TWFZNI\Wang 等 - 2025 - Graph Foundation Models A Comprehensive Survey.pdf}
}

@inproceedings{Wang2021BagOT,
  title={Bag of Tricks for Node Classification with Graph Neural Networks},
  author={Yangkun Wang and Jiarui Jin and Weinan Zhang and Yong Yu and Zheng Zhang and David Paul Wipf},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235353052}
}
@inproceedings{Wu2019SimplifyingGC,
  title={Simplifying Graph Convolutional Networks},
  author={Felix Wu and Tianyi Zhang and Amauri H. de Souza and Christopher Fifty and Tao Yu and Kilian Q. Weinberger},
  booktitle={International Conference on Machine Learning},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:67752026}
}
@article{Kim2022PureTA,
  title={Pure Transformers are Powerful Graph Learners},
  author={Jinwoo Kim and Tien Dat Nguyen and Seonwoo Min and Sungjun Cho and Moontae Lee and Honglak Lee and Seunghoon Hong},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.02505},
  url={https://api.semanticscholar.org/CorpusID:250311113}
}
@article{Buterez2024AnEA,
  title={An end-to-end attention-based approach for learning on graphs},
  author={David Buterez and Jon Paul Janet and Dino Oglic and Pietro Li{\'o}},
  journal={Nature Communications},
  year={2024},
  volume={16},
  url={https://api.semanticscholar.org/CorpusID:267740349}
}
@Article{MGT,
author ="Anselmi, Marco and Slabaugh, Greg and Crespo-Otero, Rachel and Di Tommaso, Devis",
title  ="Molecular graph transformer: stepping beyond ALIGNN into long-range interactions",
journal  ="Digital Discovery",
year  ="2024",
volume  ="3",
issue  ="5",
pages  ="1048-1057",
publisher  ="RSC",
doi  ="10.1039/D4DD00014E",
url  ="http://dx.doi.org/10.1039/D4DD00014E",
abstract  ="Graph Neural Networks (GNNs) have revolutionized material property prediction by learning directly from the structural information of molecules and materials. However{,} conventional GNN models rely solely on local atomic interactions{,} such as bond lengths and angles{,} neglecting crucial long-range electrostatic forces that affect certain properties. To address this{,} we introduce the Molecular Graph Transformer (MGT){,} a novel GNN architecture that combines local attention mechanisms with message passing on both bond graphs and their line graphs{,} explicitly capturing long-range interactions. Benchmarking on MatBench and Quantum MOF (QMOF) datasets demonstrates that MGT{'}s improved understanding of electrostatic interactions significantly enhances the prediction accuracy of properties like exfoliation energy and refractive index{,} while maintaining state-of-the-art performance on all other properties. This breakthrough paves the way for the development of highly accurate and efficient materials design tools across diverse applications."
}

@article{Maziarka2020MoleculeAT,
  title={Molecule Attention Transformer},
  author={Lukasz Maziarka and Tomasz Danel and Slawomir Mucha and Krzysztof Rataj and Jacek Tabor and Stanislaw Jastrzebski},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.08264},
  url={https://api.semanticscholar.org/CorpusID:211171794}
}
@article{Zhu2024GraphCLIPET,
  title={GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs},
  author={Yun Zhu and Haizhou Shi and Xiaotang Wang and Yongchao Liu and Yaoke Wang and Boci Peng and Chuntao Hong and Siliang Tang},
  journal={Proceedings of the ACM on Web Conference 2025},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273346565}
}
@article{Li2025AreLL,
  title={Are Large Language Models In-Context Graph Learners?},
  author={Jintang Li and Ruofan Wu and Yuchang Zhu and Huizhe Zhang and Liang Chen and Zibin Zheng},
  journal={ArXiv},
  year={2025},
  volume={abs/2502.13562},
  url={https://api.semanticscholar.org/CorpusID:276449669}
}
@article{Sun2025GraphICLUG,
  title={GraphICL: Unlocking Graph Learning Potential in LLMs through Structured Prompt Design},
  author={Yuanfu Sun and Zhengnan Ma and Yi Fang and Jing Ma and Qiaoyu Tan},
  journal={ArXiv},
  year={2025},
  volume={abs/2501.15755},
  url={https://api.semanticscholar.org/CorpusID:275920894}
}
@article{Zhang2023GraphToolFormerTE,
  title={Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT},
  author={Jiawei Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11116},
  url={https://api.semanticscholar.org/CorpusID:258291494}
}
@article{Yang2024GraphAgentAG,
  title={GraphAgent: Agentic Graph Language Assistant},
  author={Yuhao Yang and Jiabin Tang and Lianghao Xia and Xingchen Zou and Yuxuan Liang and Chao Huang},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.17029},
  url={https://api.semanticscholar.org/CorpusID:274982759}
}
@article{Lin2024LangGFMAL,
  title={LangGFM: A Large Language Model Alone Can be a Powerful Graph Foundation Model},
  author={Tianqianjin Lin and Pengwei Yan and Kaisong Song and Zhuoren Jiang and Yangyang Kang and Jun Lin and Weikang Yuan and Junjie Cao and Changlong Sun and Xiaozhong Liu},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.14961},
  url={https://api.semanticscholar.org/CorpusID:273502813}
}
@article{Yu2023MultiGPromptFM,
  title={MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs},
  author={Xingtong Yu and Chang Zhou and Yuan Fang and Xinming Zhang},
  journal={Proceedings of the ACM Web Conference 2024},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:266052614}
}
@article{Jiang2021ContrastivePO,
  title={Contrastive Pre-Training of GNNs on Heterogeneous Graphs},
  author={Xunqiang Jiang and Yuanfu Lu and Yuan Fang and Chuan Shi},
  journal={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:240230682}
}
@article{Yu2024NonHomophilicGP,
  title={Non-Homophilic Graph Pre-Training and Prompt Learning},
  author={Xing-Xing Yu and Jie Zhang and Yuan Fang and Renhe Jiang},
  journal={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:271924424}
}
@article{Liu2023GraphPromptUP,
  title={GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks},
  author={Zemin Liu and Xingtong Yu and Yuan Fang and Xinming Zhang},
  journal={Proceedings of the ACM Web Conference 2023},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256900867}
}
@article{Rong2020GROVERSM,
  title={GROVER: Self-supervised Message Passing Transformer on Large-scale Molecular Data},
  author={Yu Rong and Yatao Bian and Tingyang Xu and Wei-yang Xie and Ying Wei and Wenbing Huang and Junzhou Huang},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.02835},
  url={https://api.semanticscholar.org/CorpusID:220363946}
}
@article{Ma2025SimplifyingGT,
  title={Simplifying Graph Transformers},
  author={Liheng Ma and Soumyasundar Pal and Yingxue Zhang and Philip H. S. Torr and Mark Coates},
  journal={ArXiv},
  year={2025},
  volume={abs/2504.12588},
  url={https://api.semanticscholar.org/CorpusID:278787442}
}
@article{Han2022GeometricallyEG,
  title={Geometrically Equivariant Graph Neural Networks: A Survey},
  author={Jiaqi Han and Yu Rong and Tingyang Xu and Wenbing Huang},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.07230},
  url={https://api.semanticscholar.org/CorpusID:246863599}
}
@inproceedings{Chen2022NAGphormerAT,
  title={NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs},
  author={Jinsong Chen and Kaiyuan Gao and Gaichao Li and Kun He},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252846362}
}
@inproceedings{Chen2022StructureAwareTF,
  title={Structure-Aware Transformer for Graph Representation Learning},
  author={Dexiong Chen and Leslie O’Bray and Karsten M. Borgwardt},
  booktitle={International Conference on Machine Learning},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:246634635}
}
@article{Hu2021OGBLSCAL,
  title={OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs},
  author={Weihua Hu and Matthias Fey and Hongyu Ren and Maho Nakata and Yuxiao Dong and Jure Leskovec},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.09430},
  url={https://api.semanticscholar.org/CorpusID:232257683}
}
@article{Mialon2021GraphiTEG,
  title={GraphiT: Encoding Graph Structure in Transformers},
  author={Gr{\'e}goire Mialon and Dexiong Chen and Margot Selosse and Julien Mairal},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.05667},
  url={https://api.semanticscholar.org/CorpusID:235390675}
}
@article{Dwivedi2020AGO,
  title={A Generalization of Transformer Networks to Graphs},
  author={Vijay Prakash Dwivedi and Xavier Bresson},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.09699},
  url={https://api.semanticscholar.org/CorpusID:229298019}
}
@inproceedings{Ying2021DoTR,
  title={Do Transformers Really Perform Badly for Graph Representation?},
  author={Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:265104899}
}
@article{Han2020ASO,
  title={A Survey on Vision Transformer},
  author={Kai Han and Yunhe Wang and Hanting Chen and Xinghao Chen and Jianyuan Guo and Zhenhua Liu and Yehui Tang and An Xiao and Chunjing Xu and Yixing Xu and Zhaohui Yang and Yiman Zhang and Dacheng Tao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  volume={PP},
  pages={1-1},
  url={https://api.semanticscholar.org/CorpusID:236986986}
}
@article{Lin2021ASO,
  title={A Survey of Transformers},
  author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  journal={AI Open},
  year={2021},
  volume={3},
  pages={111-132},
  url={https://api.semanticscholar.org/CorpusID:235368340}
}
@article{ElKishky2024OpenAIOS,
  title={OpenAI o1 System Card},
  author={Ahmed El-Kishky},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.16720},
  url={https://api.semanticscholar.org/CorpusID:272648256}
}
@article{Guo2025G1TL,
  title={G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning},
  author={Xiaojun Guo and Ang Li and Yifei Wang and Stefanie Jegelka and Yisen Wang},
  journal={ArXiv},
  year={2025},
  volume={abs/2505.18499},
  url={https://api.semanticscholar.org/CorpusID:278904995}
}
@article{DeepSeekAI2025DeepSeekR1IR,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Jun-Mei Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiaoling Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bing-Li Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dong-Li Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and JingChang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Jiong Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and M. Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and Ruiqi Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shao-Kang Wu and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wen-Xia Yu and Wentao Zhang and Wangding Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyu Jin and Xi-Cheng Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yi Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yu-Jing Zou and Yujia He and Yunfan Xiong and Yu-Wei Luo and Yu-mei You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanping Huang and Yao Li and Yi Zheng and Yuchen Zhu and Yunxiang Ma and Ying Tang and Yukun Zha and Yuting Yan and Zehui Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhen-guo Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zi-An Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
  journal={ArXiv},
  year={2025},
  volume={abs/2501.12948},
  url={https://api.semanticscholar.org/CorpusID:275789950}
}
@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:13756489}
}
@article{Wang2020GCNRLCD,
  title={GCN-RL Circuit Designer: Transferable Transistor Sizing with Graph Neural Networks and Reinforcement Learning},
  author={Hanrui Wang and Kuan Wang and Jiacheng Yang and Linxiao Shen and Nan Sun and Hae-Seung Lee and Song Han},
  journal={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  year={2020},
  pages={1-6},
  url={https://api.semanticscholar.org/CorpusID:218470023}
}
@article{Li2025CanOD,
  title={Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning},
  author={Yu Li and Zhuoshi Pan and Honglin Lin and Mengyuan Sun and Conghui He and Lijun Wu},
  journal={ArXiv},
  year={2025},
  volume={abs/2507.17512},
  url={https://api.semanticscholar.org/CorpusID:280219840}
}
@article{Mirhoseini2021AGP,
  title={A graph placement methodology for fast chip design},
  author={Azalia Mirhoseini and Anna Goldie and Mustafa Yazgan and Joe Wenjie Jiang and Ebrahim M. Songhori and Shen Wang and Young-Joon Lee and Eric Johnson and Omkar Pathak and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and Will Hang and Emre Tuncer and Quoc V. Le and James Laudon and Richard Ho and Roger Carpenter and Jeff Dean},
  journal={Nature},
  year={2021},
  volume={594},
  pages={207 - 212},
  url={https://api.semanticscholar.org/CorpusID:235395490}
}
@inproceedings{You2018GraphCP,
  title={Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation},
  author={Jiaxuan You and Bowen Liu and Rex Ying and Vijay S. Pande and Jure Leskovec},
  booktitle={Neural Information Processing Systems},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:46978626}
}
@article{Khalil2017LearningCO,
  title={Learning Combinatorial Optimization Algorithms over Graphs},
  author={Elias Boutros Khalil and Hanjun Dai and Yuyu Zhang and Bistra N. Dilkina and Le Song},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.01665},
  url={https://api.semanticscholar.org/CorpusID:3486660}
}
@inproceedings{Song2018Structure2vecDL,
  title={Structure2vec: Deep Learning for Security Analytics over Graphs},
  author={Le Song},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:70031433}
}
@article{Bello2016NeuralCO,
  title={Neural Combinatorial Optimization with Reinforcement Learning},
  author={Irwan Bello and Hieu Pham and Quoc V. Le and Mohammad Norouzi and Samy Bengio},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.09940},
  url={https://api.semanticscholar.org/CorpusID:3649804}
}
@article{Sun2023LargeLM,
  title={Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs},
  author={Shengyin Sun and Yuxiang Ren and Chen Ma and Xuecang Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.14324},
  url={https://api.semanticscholar.org/CorpusID:265445904}
}
@article{Zhao2022LearningOL,
  title={Learning on Large-scale Text-attributed Graphs via Variational Inference},
  author={Jianan Zhao and Meng Qu and Chaozhuo Li and Hao Yan and Qian Liu and Rui Li and Xing Xie and Jian Tang},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.14709},
  url={https://api.semanticscholar.org/CorpusID:253117079}
}
@inproceedings{Lin2024GT2VecLL,
  title={GT2Vec: Large Language Models as Multi-Modal Encoders for Text and Graph-Structured Data},
  author={Jiacheng Lin and Kun Qian and Haoyu Han and Nurendra Choudhary and Tianxin Wei and Zhongruo Wang and Sahika Genc and E-Wen Huang and Sheng Wang and Karthik Subbian and Danai Koutra and Jimeng Sun},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273350888}
}
@article{Brannon2023ConGraTSC,
  title={ConGraT: Self-Supervised Contrastive Pretraining for Joint Graph and Text Embeddings},
  author={William Brannon and Suyash Pradeep Fulay and Hang Jiang and Wonjune Kang and Brandon Cain Roy and Jad Kabbara and Dwaipayan Roy},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.14321},
  url={https://api.semanticscholar.org/CorpusID:258841509}
}
@article{Reimers2019SentenceBERTSE,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Nils Reimers and Iryna Gurevych},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.10084},
  url={https://api.semanticscholar.org/CorpusID:201646309}
}
@article{Hamilton2017InductiveRL,
  title={Inductive Representation Learning on Large Graphs},
  author={William L. Hamilton and Zhitao Ying and Jure Leskovec},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.02216},
  url={https://api.semanticscholar.org/CorpusID:4755450}
}
@article{Wang2023CanLM,
  title={Can Language Models Solve Graph Problems in Natural Language?},
  author={Heng Wang and Shangbin Feng and Tianxing He and Zhaoxuan Tan and Xiaochuang Han and Yulia Tsvetkov},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.10037},
  url={https://api.semanticscholar.org/CorpusID:258740923}
}
@article{Monti2016GeometricDL,
  title={Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs},
  author={Federico Monti and Davide Boscaini and Jonathan Masci and Emanuele Rodol{\`a} and Jan Svoboda and Michael M. Bronstein},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={5425-5434},
  url={https://api.semanticscholar.org/CorpusID:301319}
}
@inproceedings{Rong2019DropEdgeTD,
  title={DropEdge: Towards Deep Graph Convolutional Networks on Node Classification},
  author={Yu Rong and Wenbing Huang and Tingyang Xu and Junzhou Huang},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:212859361}
}
@inproceedings{AbuElHaija2019MixHopHG,
  title={MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing},
  author={Sami Abu-El-Haija and Bryan Perozzi and Amol Kapoor and Hrayr Harutyunyan and Nazanin Alipourfard and Kristina Lerman and Greg Ver Steeg and A. G. Galstyan},
  booktitle={International Conference on Machine Learning},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:141460877}
}@article{Xu2018RepresentationLO,
  title={Representation Learning on Graphs with Jumping Knowledge Networks},
  author={Keyulu Xu and Chengtao Li and Yonglong Tian and Tomohiro Sonobe and Ken-ichi Kawarabayashi and Stefanie Jegelka},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.03536},
  url={https://api.semanticscholar.org/CorpusID:47018956}
}
@article{Chen2021EdgeFeaturedGA,
  title={Edge-Featured Graph Attention Network},
  author={Jun Chen and Hao-peng Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.07671},
  url={https://api.semanticscholar.org/CorpusID:231639154}
}
@article{Brody2021HowAA,
  title={How Attentive are Graph Attention Networks?},
  author={Shaked Brody and Uri Alon and Eran Yahav},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.14491},
  url={https://api.semanticscholar.org/CorpusID:235254358}
}
@article{Wu2019ACS,
  title={A Comprehensive Survey on Graph Neural Networks},
  author={Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2019},
  volume={32},
  pages={4-24},
  url={https://api.semanticscholar.org/CorpusID:57375753}
}
@inproceedings{Defferrard2016ConvolutionalNN,
  title={Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  author={Micha{\"e}l Defferrard and Xavier Bresson and Pierre Vandergheynst},
  booktitle={Neural Information Processing Systems},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:3016223}
}
@article{Kipf2016SemiSupervisedCW,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Thomas Kipf and Max Welling},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.02907},
  url={https://api.semanticscholar.org/CorpusID:3144218}
}
@article{Velickovic2017GraphAN,
  title={Graph Attention Networks},
  author={Petar Velickovic and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Lio’ and Yoshua Bengio},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.10903},
  url={https://api.semanticscholar.org/CorpusID:3292002}
}
@article{Wang2021LearningIB,
  title={Learning Intents behind Interactions with Knowledge Graph for Recommendation},
  author={Xiang Wang and Tinglin Huang and Dingxian Wang and Yancheng Yuan and Zhenguang Liu and Xiangnan He and Tat-seng Chua},
  journal={Proceedings of the Web Conference 2021},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231924585}
}
@article{Liu_Wang_Vu_Moretti_Bodenheimer_Meiler_Derr_2023, title={Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26679}, DOI={10.1609/aaai.v37i12.26679}, abstractNote={In computer-aided drug discovery, quantitative structure activity relation models are trained to predict biological activity from chemical structure. Despite the recent success of applying graph neural network to this task, important chemical information such as molecular chirality is ignored. To fill this crucial gap, we propose Molecular-Kernel Graph NeuralNetwork (MolKGNN) for molecular representation learning, which features SE(3)-/conformation invariance, chirality-awareness, and interpretability. For our MolKGNN, we first design a molecular graph convolution to capture the chemical pattern by comparing the atom’s similarity with the learnable molecular kernels. Furthermore, we propagate the similarity score to capture the higher-order chemical pattern. To assess the method, we conduct a comprehensive evaluation with nine well-curated datasets spanning numerous important drug targets that feature realistic high class imbalance and it demonstrates the superiority of MolKGNN over other graph neural networks in computer-aided drug discovery. Meanwhile, the learned kernels identify patterns that agree with domain knowledge, confirming the pragmatic interpretability of this approach. Our code and supplementary material are publicly available at https://github.com/meilerlab/MolKGNN.}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Yunchao (Lance) and Wang, Yu and Vu, Oanh and Moretti, Rocco and Bodenheimer, Bobby and Meiler, Jens and Derr, Tyler}, year={2023}, month={Jun.}, pages={14356-14364} }
@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Ma-teusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}
@article{Alon2020OnTB,
  title={On the Bottleneck of Graph Neural Networks and its Practical Implications},
  author={Uri Alon and Eran Yahav},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.05205},
  url={https://api.semanticscholar.org/CorpusID:219558760}
}
@article{Rusch2023ASO,
  title={A Survey on Oversmoothing in Graph Neural Networks},
  author={T. Konstantin Rusch and Michael M. Bronstein and Siddhartha Mishra},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.10993},
  url={https://api.semanticscholar.org/CorpusID:257632346}
}
@inproceedings{Llama,
  title={The Llama 3 Herd of Models},
  author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur'elien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Rozi{\`e}re and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cris-tian Cant{\'o}n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr{\'e}goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and Kenneth Heafield and Kevin R. Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Ma-hesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melissa Hall Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Niko-lay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi{\'c} and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ron-nie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sa-hana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Chandra Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Vir-ginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto de Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm’an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthias Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Mun-ish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navy-ata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pe-dro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll{\'a}r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Kumar Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Andrei Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:271571434}
}
@article{T5,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={21},
  pages={140:1-140:67},
  url={https://api.semanticscholar.org/CorpusID:204838007}
}
@inproceedings{BERT,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}
@article{Zhao2023ASO,
  title={A Survey of Large Language Models},
  author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Z. Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jianyun Nie and Ji-rong Wen},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.18223},
  url={https://api.semanticscholar.org/CorpusID:257900969}
}
@article{Masters2022GPSAO,
  title={GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction},
  author={Dominic Masters and Josef Dean and Kerstin Klaser and Zhiyi Li and Sam Maddrell-Mander and Adam Sanders and Hatem Helal and Deniz Beker and Ladislav Ramp{\'a}{\vs}ek and D. Beaini},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.02229},
  url={https://api.semanticscholar.org/CorpusID:254247184}
}
@article{Rampek2022RecipeFA,
  title={Recipe for a General, Powerful, Scalable Graph Transformer},
  author={Ladislav Rampášek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and D. Beaini},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.12454},
  url={https://api.semanticscholar.org/CorpusID:249062808}
}
@article{Anselmi2024MolecularGT,
  title={Molecular Graph Transformer: Stepping Beyond ALIGNN Into Long-Range Interactions},
  author={Marco Anselmi and Greg Slabaugh and Rachel Crespo-Otero and Devis Di Tommaso},
  journal={Digital Discovery},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269351551}
}
@inproceedings{Guo2022GraphbasedMR,
  title={Graph-based Molecular Representation Learning},
  author={Zhichun Guo and Bozhao Nan and Yijun Tian and O. Wiest and Chuxu Zhang and N. Chawla},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:250426584}
}
@inproceedings{社交网络分析2,
author = {Zhang, Yanfu and Gao, Hongchang and Pei, Jian and Huang, Heng},
title = {Robust Self-Supervised Structural Graph Neural Network for Social Network Prediction},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512182},
doi = {10.1145/3485447.3512182},
abstract = {The self-supervised graph representation learning has achieved much success in recent web based research and applications, such as recommendation system, social networks, and anomaly detection. However, existing works suffer from two problems. Firstly, in social networks, the influential neighbors are important, but the overwhelming routine in graph representation-learning utilizes the node-wise similarity metric defined on embedding vectors that cannot exactly capture the subtle local structure and the network proximity. Secondly, existing works implicitly assume a universal distribution across datasets, which presumably leads to sub-optimal models considering the potential distribution shift. To address these problems, in this paper, we learn structural embeddings in which the proximity is characterized by 1-Wasserstein distance. We propose a distributionally robust self-supervised graph neural network framework to learn the representations. More specifically, in our method, the embeddings are computed based on subgraphs centering at the node of interest and represent both the node of interest and its neighbors, which better preserves the local structure of nodes. To make our model end-to-end trainable, we adopt a deep implicit layer to compute the Wasserstein distance, which can be formulated as a differentiable convex optimization problem. Meanwhile, our distributionally robust formulation explicitly constrains the maximal diversity for matched queries and keys. As such, our model is insensitive to the data distributions and has better generalization abilities. Extensive experiments demonstrate that the graph encoder learned by our approach can be utilized for various downstream analyses, including node classification, graph classification, and top-k similarity search. The results show our algorithm outperforms state-of-the-art baselines, and the ablation study validates the effectiveness of our design.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1352–1361},
numpages = {10},
keywords = {Graph neural networks, non-Euclidean distance, self-supervised learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}
@article{社交网络分析1,
  title={Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting},
  author={Zezhi Shao and Zhao Zhang and Fei Wang and Yongjun Xu},
  journal={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:249889716}
}

@article{Perozzi2014DeepWalkOL,
  title={DeepWalk: online learning of social representations},
  author={Bryan Perozzi and Rami Al-Rfou and Steven S. Skiena},
  journal={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:3051291}
}
@inproceedings{有向图高阶传递性嵌入,
author = {Ou, Mingdong and Cui, Peng and Pei, Jian and Zhang, Ziwei and Zhu, Wenwu},
title = {Asymmetric Transitivity Preserving Graph Embedding},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939751},
doi = {10.1145/2939672.2939751},
abstract = {Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1105–1114},
numpages = {10},
keywords = {asymmetric transitivity, directed graph, graph embedding, high-order proximity},
location = {San Francisco, California, USA},
series = {KDD '16}
}
@inproceedings{高阶邻接嵌入,
author = {Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},
title = {GraRep: Learning Graph Representations with Global Structural Information},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806512},
doi = {10.1145/2806416.2806512},
abstract = {In this paper, we present {GraRep}, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. as well as the skip-gram model with negative sampling of Mikolov et al.We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {891–900},
numpages = {10},
keywords = {algorithms, experimentation},
location = {Melbourne, Australia},
series = {CIKM '15}
}
@article{Grover2016node2vecSF,
  title={node2vec: Scalable Feature Learning for Networks},
  author={Aditya Grover and Jure Leskovec},
  journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:207238980}
}

@article{Tang2015LINELI,
  title={LINE: Large-scale Information Network Embedding},
  author={Jian Tang and Meng Qu and Mingzhe Wang and Ming Zhang and Jun Yan and Qiaozhu Mei},
  journal={Proceedings of the 24th International Conference on World Wide Web},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:8399404}
}
@article{图同构检测,
  title={Wasserstein Weisfeiler-Lehman Graph Kernels},
  author={Matteo Togninalli and M. Elisabetta Ghisu and Felipe Llinares-L{\'o}pez and Bastian Alexander Rieck and Karsten M. Borgwardt},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.01277},
  url={https://api.semanticscholar.org/CorpusID:174798401}
}
@article{标签传播,
  title = {Near linear time algorithm to detect community structures in large-scale networks},
  author = {Raghavan, Usha Nandini and Albert, R\'eka and Kumara, Soundar},
  journal = {Phys. Rev. E},
  volume = {76},
  issue = {3},
  pages = {036106},
  numpages = {11},
  year = {2007},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.76.036106},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.76.036106}
}

@article{Blondel2008FastUO,
  title={Fast unfolding of communities in large networks},
  author={Vincent D. Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  year={2008},
  volume={2008},
  pages={P10008},
  url={https://api.semanticscholar.org/CorpusID:334423}
}

@article{谱聚类,
  title={On Spectral Clustering: Analysis and an algorithm},
  author={ Ng, Andrew Y.  and  Jordan, Michael I.  and  Weiss, Yair },
  journal={proc nips},
  year={2002},
}

@article{
随机游走,
author = {Martin Rosvall  and Carl T. Bergstrom },
title = {Maps of random walks on complex networks reveal community structure},
journal = {Proceedings of the National Academy of Sciences},
volume = {105},
number = {4},
pages = {1118-1123},
year = {2008},
doi = {10.1073/pnas.0706851105},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0706851105},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0706851105},
abstract = {To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of \&gt;6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the network—including physics, chemistry, molecular biology, and medicine—information flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences.}}
@article{Hu2020OpenGB,
  title={Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  author={Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00687},
  url={https://api.semanticscholar.org/CorpusID:218487328}
}
@article{He2020LightGCNSA,
  title={LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation},
  author={Xiangnan He and Kuan Deng and Xiang Wang and Yan Li and Yongdong Zhang and Meng Wang},
  journal={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211043589}
}
@article{Zheng2019GMANAG,
  title={GMAN: A Graph Multi-Attention Network for Traffic Prediction},
  author={Chuanpan Zheng and Xiaoliang Fan and Cheng Wang and Jianzhong Qi},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08415},
  url={https://api.semanticscholar.org/CorpusID:208158373}
}
@inproceedings{DeepGate,
author = {Li, Min and Khan, Sadaf and Shi, Zhengyuan and Wang, Naixing and Yu, Huang and Xu, Qiang},
title = {DeepGate: learning neural representations of logic gates},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530497},
doi = {10.1145/3489517.3530497},
abstract = {Applying deep learning (DL) techniques in the electronic design automation (EDA) field has become a trending topic. Most solutions apply well-developed DL models to solve specific EDA problems. While demonstrating promising results, they require careful model tuning for every problem. The fundamental question on "How to obtain a general and effective neural representation of circuits?" has not been answered yet. In this work, we take the first step towards solving this problem. We propose DeepGate, a novel representation learning solution that effectively embeds both logic function and structural information of a circuit as vectors on each gate. Specifically, we propose transforming circuits into unified and-inverter graph format for learning and using signal probabilities as the supervision task in DeepGate. We then introduce a novel graph neural network that uses strong inductive biases in practical circuits as learning priors for signal probability prediction. Our experimental results show the efficacy and generalization capability of DeepGate.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {667–672},
numpages = {6},
keywords = {graph neural networks, logic gates, representation learning},
location = {San Francisco, California},
series = {DAC '22}
}
@article{eFraudCom,
author = {Zhang, Ge and Li, Zhao and Huang, Jiaming and Wu, Jia and Zhou, Chuan and Yang, Jian and Gao, Jianliang},
title = {eFraudCom: An E-commerce Fraud Detection System via Competitive Graph Neural Networks},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3474379},
doi = {10.1145/3474379},
abstract = {With the development of e-commerce, fraud behaviors have been becoming one of the biggest threats to the e-commerce business. Fraud behaviors seriously damage the ranking system of e-commerce platforms and adversely influence the shopping experience of users. It is of great practical value to detect fraud behaviors on e-commerce platforms. However, the task is non-trivial, since the adversarial action taken by fraudsters. Existing fraud detection systems used in the e-commerce industry easily suffer from performance decay and can not adapt to the upgrade of fraud patterns, as they take already known fraud behaviors as supervision information to detect other suspicious behaviors.In this article, we propose a competitive graph neural networks (CGNN)-based fraud detection system (eFraudCom) to detect fraud behaviors at one of the largest e-commerce platforms, “Taobao”1. In the eFraudCom system, (1) the competitive graph neural networks (CGNN) as the core part of eFraudCom can classify behaviors of users directly by modeling the distributions of normal and fraud behaviors separately; (2) some normal behaviors will be utilized as weak supervision information to guide the CGNN to build the profile for normal behaviors that are more stable than fraud behaviors. The algorithm dependency on fraud behaviors will be eliminated, which enables eFraudCom to detect fraud behaviors in presence of the new fraud patterns; (3) the mutual information regularization term can maximize the separability between normal and fraud behaviors to further improve CGNN. eFraudCom is implemented into a prototype system and the performance of the system is evaluated by extensive experiments. The experiments on two Taobao and two public datasets demonstrate that the proposed deep framework CGNN is superior to other baselines in detecting fraud behaviors. A case study on Taobao datasets verifies that CGNN is still robust when the fraud patterns have been upgraded.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {47},
numpages = {29},
keywords = {Online e-commerce platforms, fraud detection system, graph neural networks}
}
@book{lamport1994latex,
  author={Leslie Lamport},
  title={\LaTeX{} A Document Preparation System: User's Guide and Reference Manual},
  year={1994},
  edition={2},
  address={Reading, Massachusetts},
  publisher={Addison-Wesley},
}

@book{takeuti1973,
  title={Axiomatic Set Theory},
  author={Gaisi Takeuti and Wilson M. Zaring},
  series={Graduate Texts in Mathematics},
  volume={8},
  editor={P. R. Halmos},
  address={Berlin},
  publisher={Springer-Verlag},
  year={1973},
}
@book{TAOCP-Vol3,
  author={Donald E. Knuth},
  title={Sorting and Searching},
  series={The Art of Computer Programming},
  volume={3},
  edition={2},
  address={New York},
  publisher={Addison-Wesley Publishers Ltd},
  year={1998},
}
@book{zh-book-1,
  author={余敏 and 刘华},
  title={出版集团研究},
  address={北京},
  year={2001},
  pages={179--193},
  language={zh},
}

@book{anwen1988,
  author={昂温, G. and 昂温, P. S.},
  title={外国出版史},
  translator={陈生铮},
  address={北京},
  publisher={中国书籍出版社},
  year={1988},
  language={zh},
}

@book{anwen1988b,
  author={昂温, G. and 昂温, P. S.},
  title={美国独立战争},
  series={世界历史丛书},
  editor={张三 and 李四},
  translator={陈生铮},
  address={北京},
  publisher={中国书籍出版社},
  year={1988},
  citedate={2013-08-30},
  url={http://www.google.com/},
  language={zh},
}

@proceedings{a2-1,
  editor={{中国力学学会}},
  title={第3届全国实验流体力学学术会议论文集},
  address={天津},
  year={1990},
  language={zh},
}

@proceedings{a2-2,
  editor={Rosenthall, E. M.},
  title={Proceedings of the Fifth Canadian Mathematical Congress,
                  University of Montreal, 1961},
  address={Toronto},
  publisher={University of Toronto Press},
  year={1963},
}


@inproceedings{nonlinear1996,
  author={钟文发},
  title={非线性规划在可燃毒物配置中的应用},
  editor ={赵玮},
  booktitle={运筹学的理论与应用：中国运筹学会第五届大会论文集},
  address={西安},
  publisher={西安电子科技大学出版社},
  year={1996},
  pages={468-471},
  language={zh},
}

@inproceedings{fourney1971,
  author={Fourney M. E.},
  title={Advances in holographic photoelasticity},
  editor={{American Society of Mechanical Engineers,
           Applied Mechanics Division}},
  booktitle={Symposium on Applications of Holography in Mechanics,
             August 23--25, 1971, University of Southern California,
             Los Angeles, California},
  address={New York},
  publisher={ASME},
  year={c1971},
  pages={17-38},
}

@inproceedings{aczel1998,
  author={Peter Aczel},
  title={On relating type theories and set theories},
  booktitle={Types `98, the proceedings of the 1998 workshop
             on Types for Proofs and Programs},
  year={1998},
  editor={T. Altenkirch and W. Naraschewski and B. Reus},
  volume={1657},
  series={Lecture Notes in Computer Science},
  pages={1-18},
  address={Kloster Irsee},
  publisher={Springer},
}

@article{kanamori1998,
  author={Kanamori, H.},
  title={Shaking without quaking},
  journal={Science},
  year={1998},
  volume={279},
  number={5359},
  pages={2063-2064},
}

@article{caplan1993,
  author={Caplan, P.},
  title={Cataloging internet resources},
  journal={The Public Access Computer Systems Review},
  year={1993},
  volume={4},
  number={2},
  pages={61-66},
}

@article{christine1998,
  author={Christine, M.},
  title={Plant physiology: plant biology in the Genome Era},
  journal={Science},
  year={1998},
  volume={281},
  pages={331-332},
  url={http://www.sciencemag.org/cgi/collection/anatmorp},
  citedate={1998-09-23},
}

@article{lixiaodong1999,
  author={李晓东 and 张庆红 and 叶瑾琳},
  title={气候学研究的若干理论问题},
  journal={北京大学学报: 自然科学版},
  year={1999},
  volume={35},
  number={1},
  pages={101-106},
  language={zh},
}


@phdthesis{1-5,
  author={孙玉文},
  title={汉语变调构词研究},
  address={北京},
  school={北京大学中文系},
  year={2000},
  language={zh},
}

@masterthesis{a4-1,
  author={张志祥},
  title={间断动力系统的随机扰动及其在守恒律方程中的应用},
  address={南京},
  school={南京大学数学学院},
  year={1998},
  language={zh},
}

@phdthesis{a4-2,
  author={Calms, R. B.},
  title={Infrared spectroscopic studies on solid oxygen},
  address={Berkeley},
  school={University of California},
  year={1965},
}

@PHDTHESIS{Anderson1993,
  author={Penny Anderson},
  title={Program Derivation by Proof Transformation},
  address={Pittsburgh, USA},
  school={Carnegie Mellon University},
  year={1993},
  citedate={2007-11-02},
  url={http://citeseer.nj.nec.com/anderson93program.html},
}

@online{pacs1989,
  title={PACS-L: the public-access computer systems forum},
  address={Houston, Tex},
  publisher={University of Houston Libraries},
  year={1989},
  url={http://info.lib.uh.edu/pacsl.html},
  citedate={1995-05-17},
}

@online{oclc2000,
  author={{Online Computer Library Center, Inc.}},
  title={History of OCLC},
  url={http://www.oclc.org/about/history/default.htm},
  citedate={2000-01-08},
}

@online{chuban2001,
  author={萧钰},
  title={出版业信息化迈入快车道},
  modifydate={2001-12-19},
  url={http://www.creader.com/news/20011219/200112190019.html},
  citedate={2002-04-15},
  langauge={zh},
}

@online{h7n9,
  author={张乐},
  title={我科学家成功研发人感染H7N9禽流感病毒疫苗株},
  address={北京},
  publisher={人民网},
  year={2013},
  modifydate={2013-10-27},
  url={http://society.people.com.cn/n/2013/1027/c1008-23337665.html},
  citedate={2013-10-27},
  langauge={zh},
}

@webpage{wikipedia_moores_law,
  title = {{Moore's law}},
  author = {{Wikipedia contributors}},
  publisher = {Wikipedia, The Free Encyclopedia},
  year = {2015},
  url = {https://en.wikipedia.org/wiki/Moore%27s_law},
  modifydate = {2015/06/14},
  citedate = {2015/06/15}
}

@patent{p6915001,
  author={Tachibana, R. and Shimizu, S.
          and Kobayshi, S. and Other Authors},
  title={Electronic watermarking method and system},
  country={US},
  patentid={6,915,001},
  date={2002-04-25},
}

@patent{p8284102,
  author={Koseki, A. and Momose, H.
          and Kawahito, M. and Other Authors},
  title={Compiler},
  country={US},
  patentid={8,284,102},
  date={2002-05-25},
}

@patent{p88105607.3,
  author={姜锡洲},
  title={一种温热外敷药制备方案},
  country={中国},
  patentid={88105607.3},
  date={1989-07-26},
  langauge={zh},
}

@patent{p01128777.2,
  author={{西安电子科技大学}},
  title={光折变自适应光外差探测方法},
  country={中国},
  patentid={01128777.2},
  date={2002-03-06},
  langauge={zh},
}

@patent{p92214985.2,
  author={刘加林},
  title={多功能一次性压舌板},
  country={中国},
  patentid={92214985.2},
  date={1993-04-14},
  language={zh},
}

@patent{p01129210.5,
  author={{河北绿洲生态环境科技有限公司}},
  title={一种荒漠化地区生态植被综合培育种植方法},
  country={中国},
  patentid={01129210.5},
  date={2001-10-24},
  language={zh},
}

@program{scheduler1983,
  title={Project scheduler},
  address={Sunnyvale, Calif.},
  publisher={Scitor Corporation},
  year={c1983},
  media={DK},
}

@program{njuthesis,
  author={胡海星},
  title={南京大学学位论文模板},
  year={2013},
  citedate={2013-08-31},
  url={https://github.com/Haixing-Hu/nju-thesis},
  language={zh},
}

@manual{ipad,
  author={{Apple Inc}},
  title={iPad User Manual},
  address={Cupertino},
  publisher={Apple Inc},
  year={2008},
}

@manual{keynotes09,
  author={{Apple Inc}},
  title={Keynote'09 使用手册},
  address={Cupertino},
  publisher={Apple Inc},
  year={2009},
  language={zh},
}

@unpublished{bove2002,
  author = {Ana Bove and Venanzio Capretta},
  title = {Modelling General Recursion in Type Theory},
  year = {2002},
  citedate={2007-10-11},
  url = {http://citeseer.nj.nec.com/bove02modelling.html},
}

@newspaper{renminribao,
  editor={{人民日报编辑部}},
  title={人民日报},
  year={2011},
  volume={22892},
  address={北京},
  publisher={人民日报出版社},
  language={zh},
}

@webpage{dubash2010,
  title = {{Moore's Law is dead, says Gordon Moore}},
  author = {Manek Dubash},
  publisher = {Techworld},
  year = {2010},
  url = {www.techworld.com/news/operating-systems/moores-law-is-dead-says-gordon-moore-3576581/},
  modifydate={2010/4/13},
}
