\chapter{基于条件对抗生成模型的数字人建模方案}

在上一章节中，本文定义了精细化的生成式数字人基准数据集构建方法，得到了能够准确表示数字人各部位的精细化合成数据表示。但如何充分利用该表示来建模对应的数字人形象还有待解决。在本节中，本文首先针对数字人的生成任务进行科学严谨的问题描述，并进一步将其转化为一个科学问题来解决。在本节中将基于数据流阐述本文基于对抗生成模型框架下的数字人生成模型的总体建模流程。

最后，针对生成式数字人任务所需要的建模数据样本，本文利用上一章提出的方法构建了本文所使用的基准数据集。同时为了进一步衡量构建的数字人模型效果，进一步阐明了本文测评数字人所使用的主要评价指标。

\section{生成式数字人建模问题描述}

本文研究重点是单目视频的生成式数字人建模，因此本文的总体研究目标描述如下。

给定一系列包含人物面部、肢体的单目视频序列 $Y_{1:n} = {Y_1,\dots,Y_n}$ ，其中$n$为视频总帧数。模型根据该视频序列进行数字人建模，输出与该单目视频中身体姿态、口型表情均保持一致的视频序列 $\widetilde{Y}_{1:n}= {\widetilde{Y}_1,\dots,\widetilde{Y}_n}$。

当完成建模后，给定一系列输入驱动数据序列 $I_{1:m} = {I_1,\dots,I_m}$ 时。需要输出具有与给定身份一致身份信息，且与驱动序列身体姿态、口型表情均保持一致的视频序列$\widetilde{Y}_{1:m}= {\widetilde{Y}_1,\dots,\widetilde{Y}_m}$。

根据上述定义，模型应在建模和驱动过程保持的输入数据源，为此在建模过程中，人物面部、肢体的单目视频序列 $Y_{1:n}$需要先映射为与驱动数据一致的序列 $I_{1:n}$ 对模型进行建模。

针对上述研究目标，结合上一章中的基准数据集构建方法，本文将生成式数字人建模拆解为如下两个子科学问题。

1)多维度驱动数据提取：输入 $n$ 帧单目视频序列 $Y_{1:n}$，输出用以驱动模型进行建模的多维度表示序列 $I_{1:n}$。

2)条件式生成模型建模: 输入 $n$ 帧多维度表示序列 $I_{1:n}$作为条件，输出与原始视频$Y_{1:n} = {Y_1,\dots,Y_n}$一致的视频序列 $\widetilde{Y}_{1:n}$。

\section{基于条件对抗生成模型数字人建模总体流程框架}

针对上述科学问题，本文提出了一套基于条件对抗生成模型的数字人技术总体解决方案。其总体框架如图\ref{fig:pipe}所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chapter2/pipe.pdf}
	\caption{总体框架图}
	\label{fig:pipe}
\end{figure}

在该架构下，通过间接的两个阶段分别解决两个子科学问题，实现数字人建模，旨在实现条件可控的针对特定身份的数字人视频生成。第一阶段中，通过基准数据集构建方法，输入人物单目视频 $Y_{1:n}$，生成建模所需的多维度合成图像序列$I_{1:n}$；第二阶段，通过构建条件对抗生成网络，以第一阶段得到的多维度合成图像序列$I_{1:n}$为条件，模型学习从多维度合成图像序列到真实人物视频帧的映射$P(Y_{1:n}|I_{1:n})$。

其中基准数据集构建方法在第二章中已经详细介绍，在下一节中将会介绍本文依s赖该方法构建的基准数据集，该数据集能够的良好应用到演讲、手语、节目制播等多个数字人生成任务场景当中。

针对条件式对抗网络的具体架构，在本文的第四章中，以 StyleUNet\cite{styleAvatar} 模型为基础作为条件对抗生成网络的具体实现，构建了逐帧映射的数字人生成模型。并在此基础上通过基于人体大模型，分别实现了基于人体大模型的损失函数和基于数字人任务的判别器结构。分别从整体形象和局部细节增强 StyleUNet 数字人建模生成的质量。在第五章中则针对基准模型中逐帧映射可能出现的时序一致性问题，分别从跨帧几何一致性和时序连续性的两个角度对该问题进行解决；提出了基于法线约束的多任务学习方法和基于连续帧的时序融合方法，两种方法均对时序改善有较大提升。

\section{面向多维度特征融合的基准数据集构建}

当前的生成式数字人方法缺少统一的数据处理标准，使得在构建完整的数字人解决方案时，
往往需要根据所使用的基础模型，修改对应的数据预处理方法；并且当驱动模态发生变化时，也难以对方法进行公平比较。
如在 Animate anyone\cite{hu2024animate} 数字人视频生成模型中，
模型使用基于DWpose\cite{dwpose} 绘制的身体点线图作为驱动模态，在 Champ\cite{zhu2024champ} 中，
额外使用了 SMPL\cite{smpl} 模型的法线图、语义图、深度图加以融合。在Magicanimate\cite{magicanimate}中，
则使用Densepose\cite{guler2018densepose} 渲染图作为驱动模态。
因此本文提出了一种细粒度高质量的以人为中心的数据预处理流程，用以从人物视频数据集当中提取包含全身多部位细粒度的合成模态数据，并进一步分析了提取出的合成模态表征能力。

\section{基准数据集介绍}

本节使用上一章提出的基准数据集构建方法构建本文使用的数据集。本文数据集来源包括演讲场景\cite{ted-talk} ，手语场景\cite{how2sign,slovo}以及本文自采集的高清播报数据集。在该节中，本文对这些数据集进行简要介绍并给出部分处理流程。

目前数字人领域常用的公开数据集有 TikTok\cite{tik-tok}，ubc-fashion\cite{dwnet}数据集，其分别针对短视频舞蹈场景和模特走秀场景。前者视频总时长较短，且视频质量良莠不齐，存在很多模糊的片段。fashion 数据集则场景极为固定，该数据集中具有服装和人物的多样性，但是缺少手部动作变化，同时也缺少数字人面部嘴型和表情的多样性。因此，本文未将以上数据集作为基准数据集的一部分。

首先在演讲场景下，本文重新处理了 Ted-talk\cite{ted-talk} 数据集，该数据集针对全身数字人生成任务。原始的 Ted-talk 数据集仅给出了半身的裁剪镜头包围盒和视频帧索引，并使用较低分辨率的视频源。本文重新从 youtube 中下载对应$1920 \times 1080$和$1280 \times 720$的高清视频，并保留原始Ted-talk给出的视频帧索引标准。
% 针对索引对应的视频帧，直接利用基准数据集构建方法的数据自动化清洗和分割，将其处理为 $1024 \times 1024$ 分辨率 25 帧每秒的
利用基准数据集处理丰富将其处理为全身数字人预处理视频。最终在ted-talk中共保留338个角色约3.5小时的数字人视频数据。

本文认为手语识别领域的数据集是对数字人生成任务的有利补充，该类数据集在其他的生成式数字人工作中被利用的很少。手语数据集往往针对人物进行半身拍摄，具有清晰的手部动作、同时会配合相应的唇形动作用以描述对应的词汇。
因此该类数据集与演讲数据集类似，生成式模型同样可以通过该类数据集学习到对应丰富的手部动作和唇形表示。针对基于泛化性的数字人任务，手语数据集中丰富的手部动作能能够增强模型的手部表现能力。针对基于特定身份的数字人任务，该类模型也能建构出良好的数字人形象表示，作为针对演讲场景或手语场景的数字人。
本文将手语识别数据集 How2sign \cite{how2sign} 和 Slovo \cite{slovo} 纳入本文的数字人基准数据集构建当中。其中 How2sign 数据集质量较高，为绿幕场景下多视角采集数据集，包含有11个手语主播形象，每个手语主播具有多种不同的服装外观，总共包含约 90 小时分辨率为 $1280 times 720$ 视频数据，本文仅采用其正面视角采集的人物视频图像，并将该数据集规模与其他数据集匹配，保留其中约10小时的数据。slovo 数据集中包含有194个手语主播形象，其采集质量则稍低，使用手机作为设备进行录制，其原始数据集中包含有$1920 \times 1358$，$768 \times 1328$等多种分辨率，slovo数据集中开头均为手语主播点开手机进行录制，该部分缺乏实际意义，并往往使得人物在视频中产生较大的位移和手部模糊，因此该部分数据被剔除，最终保留约8小时的手语数据集。

本文针对数字人任务同样采集了高质量超清数据集。其主要面向新闻播报场景，其中包含有超过50个人物身份，视频总时长超过10小时。根据采集半身人物形象和全身人物形象不同，该数据集均采用 $3840 \times 2160$ 或 $2160 \times 3840$ 的 4K 分辨率以每秒50帧以上的速率进行采集。此外自采集数据集中还包含了不同的采集场景，主要可分为室内恒定光照恒定镜头下的绿幕采集视频和可变光照可变镜头的室外采集视频。

最终，本文将上述数据集通过第二章提出的数字人基准数据集构建方法进行多维度数据和合成图像的生成，作为本文的基准数据集。通过此流程处理完成的数据集如表\ref{tab:datasets_comparison}所示。在后续的章节中，本文使用了该基准数据集的子集作为实验数据集。
% 数据集场景 人物类型 原始分辨率 处理分辨率 总视频时长 角色数量
\begin{table}[!htbp]
    \centering
    \caption{本文采集的数据集}
    \label{tab:datasets_comparison}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    数据集场景          & 人物类型          & 处理分辨率            & 处理后总时长    & 角色数量 \\ \midrule
    演讲（Ted-Talk）    & 全身              & $1024 \times 1024$ &  3.5小时     & 338     \\
    手语（How2sign）    & 半身              & $1024 \times 1024$ & 10小时     & 11      \\
    手语（Slovo）       & 半身              & $1024 \times 1024$ & 8小时     & 194     \\
    新闻播报            & 半身/全身人物     & $1024 \times 1024$ & 10小时     & 52     \\ \bottomrule
    \end{tabular}
    \end{table}


\section{数字人模型质量评估}

当模型完成建模任务后，需要利用一系列的定量、定性指标评估方法衡量生成式数字人模型生成的视频质量。

本文从视频和图像质量两大方面对数字人生成模型的性能进行评估，具体可以被分为像素质量评估和感知质量评估。
% 此外，本文使用基于视频质量的平均主观评分（Mean Opinion Score，MOS）的主观测评方法\cite{huynh2010study}，将人的主观感知作为数字人生成质量评估的有力补充。在本文中不少于10人对数字人形象与真人进行5分的相似度评分，再去除一个最高分和最低分后取多人平均分作为结果。

\subsection{像素质量}
像素级质量评估直接利用生成视频与真实视频之间的像素差值进行指标计算，用于直接评估生成视频与真实视频在图像像素值上的误差大小。

\begin{itemize}
    \item 平均像素误差APD(Average Pixel Distance)\cite{head2head++} 直接衡量生成图像与真实图像之间的像素误差。为生成图像和真实图像之间像素颜色的均方误差。
    其使用8位RGB表示进行计算，对于一个大小为$H \times W$的图像 $S$ 和生成图像 $T$ ，APD的计算公式如（\ref{APD}）所示。APD越小，表示生成图像和真实图像越相似。
    \begin{equation}
        \text{APD}(T, S) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \| T_{ij} - S_{ij} \|_2
        \label{APD}
    \end{equation}
    在本文中，生成图像的背景在生成过程中差异过小，因此为了进一步衡量人物实际的重建效果，使用上述数据处理过程的SAM2人物前景掩码 $M$ 对APD进行加权计算，将掩码部分权重设置为0，
    非掩码部分权重设置为1，得到人物部分的APD指标，称之为MAPD（ Masked Average Pixel Distance）平均掩码像素距离\cite{head2head++}。MAPD的计算公式如（\ref{MAPD}）所示。
    MAPD越小，表示生成图像和真实图像越相似。
    \begin{equation}
        \text{MAPD}(T, S, M) = \frac{1}{H \times W} {\sum_{i=1}^{H} \sum_{j=1}^{W} M_{ij}}\| T_{ij} - S_{ij} \|_2
        \label{MAPD}
    \end{equation}

    \item 峰值信噪比PSNR（Peak Signal-to-Noise Ratio\cite{antkowiak2000final} 是最广泛使用的衡量生成图像结果的指标方法。该指标用以衡量图像峰值信号的能量与噪声的平均能量之比。
    通常表示的时候取 $log$ 用分贝作为结果表示，公式（\ref{PSNR}）表示如下。其中 $MSE$ 即是平均像素误差项，$MAX_I$ 表示像素最大值，PSNR值越大，图像质量越高。
    %一般来说，该指标计算的结果大于40可认为图像质量好，30-40则图像质量可接受； 20-30则图像质量差；低于20表示不可接受。
    \begin{equation}
        \text{PSNR} = 10 \cdot log_{10}(\frac{MAX_I^2}{MSE})
        \label{PSNR}
    \end{equation}

    \item 结构相似性指数SSIM（Structural Similarity Index\cite{ssim} 基于人类实际视觉系统的特性，即对空间频率较低的对比差异敏感度较高，对亮度对比差异的敏感度较色度高，
    对一个区域的感知结果会受到其周围邻近区域的影响。于是结构相似性指标首先将RGB色彩空间转换到HVS空间来进行评估。并综合考虑了亮度，对比度和结构信息。其中亮度用均值来表示，
    对比度为均值归一化的方差表示，结构则表示为相关系数即统计意义上的协方差与方差乘积比值。对于样本$x$和样本$y$，其计算公式如（\ref{eq:SSIM}）所示，
    其中$\mu_x$为$x$的均值，$\mu_y$为y的均值，$\sigma_x^2$为$x$的方差，$\sigma_{y}^2$为$x$的方差，$\sigma_{xy}$为$x$和$y$的协方差，$c_1$，$c_2$为两个常数，
    避免除零。SSIM的取值范围为$(0,1]$，值越大表示两个图像越相似。
    \begin{equation}
        \text{SSIM(x,y)} = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy}+c_2)}{(\mu_x^2+\mu_y^2+c_2)}
        \label{eq:SSIM}
    \end{equation}
\end{itemize}

\subsection{感知质量}
感知质量评估则基于神经网络提取特征，根据特征差异评估生成视频与真实视频，模拟人类视觉感知上的接近程度。

弗雷歇视频距离 FVD（Fréchet Video Distance）用于衡量生成视频与真实视频在时间序列上的相似度，是基于FID\cite{heusel2017gans}扩展的视频质量评估指标，FVD值越小，表示生成视频与真实视频的特征分布越接近。
该指标使用预训练的Inflated-3D（I3D）\cite{i3d}网络从视频片段中提取特征，然后计算两组视频的特征向量的均值和协方差矩阵，使用Fréchet距离来度量它们之间的差异，计算公式（\ref{eq:fvd}）如下所示：
\begin{equation}
    \text{FVD}(\mu_r, \Sigma_r, \mu_g, \Sigma_g) = \left\| \mu_r - \mu_g \right\|_2^2 + \text{Tr}\left( \Sigma_r + \Sigma_g - 2\left( \Sigma_r^{1/2} \Sigma_g \Sigma_r^{1/2} \right)^{1/2} \right)
\label{eq:fvd}
\end{equation}

其中各项表示如下：
\begin{enumerate}
    \item $\mu_r$: 真实视频的特征向量的均值。
    \item $\Sigma_r$: 真实视频的特征向量的协方差矩阵。
    \item $\mu_g$: 生成视频的特征向量的均值。
    \item $\Sigma_g$: 生成视频的特征向量的协方差矩阵。
    \item $\left\| \mu_r - \mu_g \right\|_2^2$: 真实视频和生成视频的均值向量之间的平方欧几里得距离。
    \item $\text{Tr}$: 矩阵的迹运算，计算矩阵对角元素的和。
    \item $\Sigma_r^{1/2}$: 真实视频协方差矩阵的平方根。
    \item $\Sigma_g^{1/2}$: 生成视频协方差矩阵的平方根。
    \item $\left( \Sigma_r^{1/2} \Sigma_g \Sigma_r^{1/2} \right)^{1/2}$: 真实视频和生成视频的协方差矩阵的几何平均。
\end{enumerate}

感知图像块相似度Learned Perceptual Image Patch Similarity(LPIPS) 是基于深度网络的线性加权距离度量，一般可使用VGG\cite{vgg}、ALEXNet\cite{alex} 等网络的多层次特征表示进行度量，
用于评估真实图像和生成图像之间的感知相似性的指标，LPIPS越小，表示两张图像的特征分布越接近，计算公式（\ref{eq:LPIPS}）如下所示。
\begin{equation}
    \text{LPIPS}(x, y) = \frac{1}{L} \sum_{l=1}^{L} w_l \| f_l(x) - f_l(y) \|_2^2
\label{eq:LPIPS}
\end{equation}

其中各项表示如下：
\begin{enumerate}
    \item $x$ 和 $y$ 分别表示待比较的两张图像，通常是生成图像 $y$ 和真实图像 $x$。
    \item $L$ 是网络中的层数，表示使用的网络总共有多少层。
    \item $f_l(x)$ 和 $f_l(y)$ 分别表示图像 $x$ 和图像 $y$ 在第 $l$ 层的特征输出。
    \item $\| f_l(x) - f_l(y) \|_2^2$ 表示两张图像在第 $l$ 层特征空间的差异，使用平方欧几里得距离。
    \item $w_l$ 是第 $l$ 层的权重，用于控制每一层对最终相似度的影响。
\end{enumerate}

\section{本章小结}