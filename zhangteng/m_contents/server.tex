\chapter{生成式数字人应用案例}

生成式数字人系统的技术落地至关重要。
一套符合实际业务需求、具备良好人机交互能力的数字人系统，不仅能够推动数字人技术应用在商业、教育、娱乐等多个行业，更能进一步加速生成式数字人技术的产业化进程。目前，市面上的部分高新人工智能企业已推出基于生成式数字人的工具和系统；然而这些工具主要集中于生成静态的站立姿态、半身数字人形象，在交互能力的支持上存在不足，导致其在吸引用户注意力和提升数字人内容趣味性等方面均面临一定挑战。

在本文第二、三、四章内容中，已经涵盖了生成式数字人建模的主要技术方案。因此本章中主要围绕在完成数字人形象建模后，如何对数字人模型进行实际应用，进一步阐明了本文多维度特征融合的的生成式数字人驱动呈现流程，形成技术闭环。

在本章的第一小节中，将形式化定义数字人应用的推理流程，重点在于以数据通路的形式阐述本文的模型如何与其他数字人技术进行交互，最终产生新的数字人音视频内容。接下来以案例的形式，分别阐述中邮消费金融公司的客服数字人应用和杭州文化广播电视集团的节目制作数字人应用，针对每个应用场景通过技术架构、交互流程两个部分阐明相关系统的技术选型和应用流程。最后本章测评了本文模型和相关技术方案在实际部署时的计算资源与时间消耗，并进一步针对应用中所需满足的时间性能和计算资源消耗两个方面，给出实际应用时的优化方案。

\section{多维度特征融合的生成式数字人推理流程}

在第二章中，本文形式化定义了生成式数字人建模阶段的科学问题。
其中在建模过程中，通过构建面向多维度特征融合的数据集工具，实现了从输入 $N$ 帧单目图像序列 $Y_{1:N}$，输出用以驱动模型的显式人体多维度合成图像序列 $I_{1:N}$。而生成式模型通过多轮次的深度学习训练将 $N$ 帧多维度表示序列 $I_{1:N}$ 映射为与原始视频 $Y_{1:N}$ 一致的图像序列 $\widetilde{Y}_{1:N}$。

因此在推理流程中，根据新的一段人体多维度合成图像序列 $I_{1:M}$ 后，模型即可生成对应的图像序列 $\widetilde{Y}_{1:M}$。本文模型可以直接实现姿态图像驱动（Pose to video）数字人的任务；同时依赖于合成新的中间多维度数据集合 $MID_{1:N}$，生成新的人物多维度合成图像序列的方式，本文的生成式数字人模型可以自然的拓展到音频驱动（Audio to video）数字人或文本驱动（Text to video）数字人任务上。三种任务的推理流程关系如图 \ref{fig:driver} 所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\textwidth]{./m_figures/chatper6/inference_pipe.pdf}
	\caption{服务型数字人交互流程}
	\label{fig:driver}
\end{figure}

具体来说，针对多维度合成图像序列 $I_{1:M}$ ，其分别由神经语义图、身体法线图、身体语义图、眼部注视图和手部图组成。在模型建模阶段，本文依赖于一系列的先进单目视觉动捕模型，得到基于 SMPLX\cite{SMPL-X:2019}、MANO\cite{mano}、Faceverse\cite{wang2022faceverse} 模型的系数表示。
其中神经语义图像和全身法线图像基于 SMPLX 模型系数重建的网格进行绘制；手部图像基于 MANO 模型系数重建的网格进行绘制；神经语义图像结合了 SMPLX 身体关节点和 Faceverse 面部关键点。而眼部注视图像则由 Faceverse 面部眼部区域关键点进行表征。以上分析可以得出，对于多维度合成图像，其绘制方法在推理阶段是通用的，要解决的问题关键是如何获得基于 SMPLX、MANO、Faceverse 模型的系数表示。
特别的，基于 MANO 网格表示进行绘制时还涉及到每帧不同相机偏移和投影问题，因此在推理过程中针对手部图像的重新绘制不通过重建 MANO 模型进行，而是通过 SMPLX 网格模型间接绘制，MANO 模型为 SMPLX 模型的子集，SMPLX 提供了与 MANO 对应的手部网格点索引集。因此通过结合 SMPLX 手部网格点和 MANO 中三角面片的组成关系，能够直接完成手部图像的绘制。

直接通过姿态图像驱动完成推理任务时，与建模任务时类似，通过单目视觉动捕模型从视频中提取 SMPLX、MANO、Faceverse 模型系数，将其中提取得到的对应的姿态系数、表情系数等结合数字人建模阶段对应人物的身份系数，全局位移系数，生成重定向后的多维度合成图像序列 $I_{1:M}$。

音频驱动数字人任务则具有更强的适用性。结合基于 SMPLX 的动作生成模型，如通过基于类似音频驱动的全身生成模型 Emage\cite{emage}能够生成生成全新的身体动作序列表示，进而生成身体法线图、身体语义图、手部图表征。而为了生成准确的面部表情和与音频同步的嘴型，针对 Faceverse 面部关键点，参照 faceformer\cite{fan2022faceformer}、codetalker\cite{codetalker} 等方法的模型结构，能够建模不同风格下的音频与特定任务面部关键点偏移量的关系，结合身体表示生成对应的神经语义图像和眼部注视图像。

而文本驱动数字人任务，可以被视为音频驱动数字人任务的拓展。本文中通过 TTS 技术生成特定人物对应的音频，并进一步进行音频风格化调整，最终与音频驱动数字人任务管线适配\cite{yanglei}。

在实际应用部分，混合应用了以上三种任务管线。其中，相关场景因为对数字人动作有较严格的要求，因此数字人动作主要从视频中预提取，可以被视为通过姿态图像驱动；而数字人面部表情的生成则依赖于本节所述的音频驱动数字人和文本驱动数字人管线。

\section{客服数字人}
\subsection{技术架构}

客服数字人主要是面向客服问答等场景的数字人系统。具体来说，客服数字人系统需要可以持续在线运行，并能够根据用户的输入实时生成并推送数字人播报内容的交互型数字人系统\cite{tianyi}。

为了实现上述需求，系统的驱动呈现流程包括离线动作建模、实时交互、视听联动三个部分。其中离线动作建模模块负责对数字人动作进行建模，实时交互模块负责处理用户的输入并生成对应的数字人视频，视听联动模块负责将数字人的语音、口型、面部表情与身体动作进行同步，从而创建真实的视听体验。具体来说，系统实现如下核心技术架构设计：

\textbf{1)离线动作建模}

为了能够实时对用户的输入做出反应，系统通过动作状态机实现数字人行为建模，结合基于本文基准数据集构建中提取的中间产生的多维度数据集合 $MID_{1:N}$（SMPLX\cite{smplx}系数，MANO\cite{mano}系数等），结合四元数线性插值与动作重定向，生成详尽的节点动作和节点动作之间的过渡动作，确保数字人行为自然流畅性的切换。
在该技术架构中，主要包括一个闲置（Idle）状态节点，和多个动作（Action）状态节点，所有动作节点均互相连接。

当数字人未进行互动时其处于闲置状态，在交互时数字人从闲置状态切换到指定动作状态。对于连续交互的场景，数字人动作将在多个动作状态之间进行切换。具体切换动作类型由交互内容决定。以实例举例，如数字人播报中需提示用户进行屏幕中指定区域操作时，从闲置状态切换为左手手指指向特定方向的动作状态。

当数字人系统交互完成时，数字人将切换回闲置状态，等待下一次交互。

\textbf{2)基于预存机制的数字人实时交互}

用户需要与客服数字人进行实时交互，因此系统围绕实时性和系统资源实施优化策略。虽然本文中模型具有良好的生成速度和友好的计算资源占用，但当用户数量增加时，对系统资源的占用也会达到不可接受的程度。因此在实际应用中将其整体分解为这包括数字人动态内容的在线推理与静态内容的离线预存。

针对静态内容，系统离线将数字人中固定的动作与话术预存成视频，并保留其对应的动作信息和对应可能需要替换内容的帧号。使得在线推流时直接能从预存视频中获取对应的数字人片段，只有当该片段需要在线生成时，才调用对应的数字人模型。需要实时生成的内容包括数字、用户姓名等特定内容，通过与视频流同步的动作状态机设计，定位需要实时生成的内容片段的帧数，使得实时生成数字人片段与离线生成片段在动作中能无缝衔接。

该部分实际结果生成时还涉及到处理音频长度和原始预存部分动作长度不一致的实际问题，因此实际定位出需要生成并替换的数字人片段时，根据音频长度进行对动作片段进行插值调整，使预存动作长度和音频长度一致。

\textbf{3)视听联动}

该部分主要实现数字人的语音、口型、面部表情与身体动作的同步，从而创建真实的视听体验。该技术需要将离线建模的数字人肢体动作与驱动数字人的口型、表情进行关联，共同生成数字人视频。

上文离线动作建模时，定义了数字人在系统中如何进行动作切换。但客服数字人还需要是通过数据库内部指定的文本和音频，驱动生成对应的面部口型。与上一节的数据通路一致，因此该部分主要基于 TTS 和 Faceformer\cite{fan2022faceformer} 技术，首先产生特定数字人在指定文本数据下的音频，并根据音频下生成对应的面部顶点偏移。将该面部顶点与预存的身体关键点进行拼接，转化为神经语义图表示，与本文提出的其他多维度合成图像进行融合拼接，得到最终多维度合成图像序列。

\subsection{交互流程}

系统中用户整体交互如流程图~\ref{fig:flow_1}~所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\textwidth]{./m_figures/chatper6/直播.pdf}
	\caption{服务型数字人交互流程}
	\label{fig:flow_1}
\end{figure}

该系统的交互流程建模如下：
\begin{enumerate}
    \item 用户进入系统后，以音频或文本的方式跟数字人进行实时交互。此时在动作图中，数字人处于循环基础动作的节点，保持闲置状态，表现为循环播放的默认视频流。
    \item 当数字人引擎前端接收到对应的输入请求时，该请求会被解析，根据用户输入的内容判断对应的数字人交互输出是否需要调用已经建模好的数字人模型进行新内容的生成，否则将直接返回数据库中预存好的音视频同步的交互状态视频，并与闲置状态进行切换。
    \item 如果需要数字人模型进行新内容生成，该请求将被发送给数字人引擎后端，后端接收到请求后，调用对应的数字人模型进行新内容的生成。
    \item 数字人引擎进行新内容生成时，在一段数字人完整内容中定位需要生成的子片段内容，仅生成子片段内容，其余可复用内容，仍然是通过数据库进行获取。该部分采用实时生成并推流的方式，防止用户等待时间过长。
    \item 数字人视频流全部生成后将切换回数字人闲置状态。
\end{enumerate}

系统的交互界面如图~\ref{fig:interface_realtime}~所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.4\textwidth]{./m_figures/chatper6/IMG_8907.PNG}
	\caption{实时交互数字人用户界面}
	\label{fig:interface_realtime}
\end{figure}
与上述系统整体交互流程定义一致，用户实际使用时通过音频或直接输入文字与系统进行交互。

\section{节目制作数字人}
\subsection{技术架构}

节目制作数字人是主要面向影视节目制作的数字人系统。节目制播型数字人系统需要面向专业的编导等用户群体。

为了实现上述需求，系统的驱动呈现流程包括数字人动作编排，口型生成和多媒体功能三个组成部分。其中数字人动作编排负责对数字人在整个生成序列中的动作进行建模，口型生成模块负责从文本或音频生成数口型和面部表情并与身体动作进行同步，从而创建真实的视听体验。多媒体功能主要对生成的视频进行直接的后处理，如绿幕背景替换，视频剪辑等。具体来说，系统采用如下技术架构：

\textbf{1)数字人动作编排}

该技术实现从单目视频片段中得到对应的数字人多维度数据集合 $MID_{1:N}$。与直播数字人中类似，通过四元数现线性插值和重定向针对多个肢体动作片段（以先验模型参数表示）进行拼接形成完整节目中数字人动作。而后再依赖于该动作特征生成对应的多维度驱动图像表示。

但在重定向与插值中与直播数字人不同，节目制作中针对全身数字人场景，数字人会在场地内进行位移，因此针对两段不同的走动片段进行直接插值会导致滑步现象的产生，导致不自然的结果。因此进行动作拼接时将只针对上身动作和相机位移进行融合，下身动作采用固定的动作模式或重复第一个动作片段中的数字人移动方式。

\textbf{2)口型生成和同步}

与直播数字人中类似，为了实现动作表达和口型表达的同步，主要基于 TTS 和 Faceformer\cite{fan2022faceformer} 技术，首先产生特定数字人在指定文本数据下的音频，并根据音频下生成对应的面部顶点偏移。将该面部顶点与预存的身体关键点进行拼接，转化为神经语义图表示，最终与其他表示共同构成最终的多维度合成图像序列。

\textbf{3)多媒体功能}

通过在基准数据集构建阶段使用 SAM2\cite{sam2} 进行人物前景分割，本文基于多模态特征融合的生成式数字人解决方案直接合成绿幕数据视频。在合成新的数字人视频时可直接通过颜色阈值法进行背景替换。同时为了支持完整节目制播，通过 FFmpeg\cite{ffmpeg} 向下提供了视频剪辑、音频融合等功能。

\subsection{交互流程}

用户与系统的主要交互如流程图~\ref{fig:flow}~所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\textwidth]{./m_figures/chatper6/节目制作.pdf}
	\caption{节目制作数字人用户交互流程}
	\label{fig:flow}
\end{figure}

该系统的交互流程可建模如下：
\begin{enumerate}
\item 用户进入数字人节目制作界面后，可进行动作序列生成、语音生成、背景替换等操作。
\item 当用户输入文本作为驱动模态时，利用指定数字人声音建模后的TTS模块，将文本转化为音频。
\item 利用基于 Faceformer 数字人头部关键点建模模型，将用户输入的音频最终转换为驱动口型关键点。
\item 当用户需要进行动作序列生成时，可选择任意数字人视频，提取对应的多维度数据集合，对于多个数字人视频提取的多维度数据集合，基于四元数线性插值动作，基于普通线性插值相机位移。
\item 结合基准数据集构建方法和重定向，将驱动口型关键点与身体关键点融合，并生成驱动用的多维度合成图像。
\item 利用得到的多维度合成图像序列驱动用户指定数字人生成。
\item 结合音视频融合，背景替换，视频剪辑，最终得到最终生成的视频。
\end{enumerate}

系统的交互界面如图~\ref{fig:interface_offline}~所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chatper6/节目制播页.png}
	\caption{节目制作数字人系统界面}
	\label{fig:interface_offline}
\end{figure}
该功能页整合集成了背景替换，动作片段选择，TTS技术，数字人视频生成等多种技术。当指定数字人模型建模完成后，可直接通过该系统端到端的制作对应的数字人视频。

% \subsection{主要功能测评与优化}

% 在本节中，主要针对客服数字人中与数字人交互的功能模块进行验证，以确保软件能够按照技术架构中的定义正常工作，通过编写测试用例，验证系统功能是否正常。同时进一步分析实际部署时具体的性能消耗，并给出具体的优化方法和结果。

% 主要功能点：
% 
\section{应用测试与优化}

在上述两节，本文详细阐述了相关生成式数字人结合具体场景的应用。但是在实际部署与应用过程中，因实时性、计算资源等因素限制，虽然本文提出的基于多维度融合的数字人算法框架具有实时性的优势，但仍需要对数字人系统中涉及的其他算法和工具进行进一步优化，以期望达到预期的效果并进一步提升系统的整体表现。而本文所构建的两个下游应用中，实际上存在大量共性的数字人工具和算法，但二者对于生成速度及系统资源消耗的限制并不一致，因此在该部分本文首先对系统中相关算法进行性能测试，进而提出存在的性能瓶颈，和实际应用过程中使用的的具体优化方案。

\begin{table}[!htbp]
    \centering
    \caption{硬件环境配置表}
    \label{tab:hardware_env3}
    \scalebox{0.9}{ 
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\textwidth}{c Y}
    \hline
    \textbf{硬件名称} & \textbf{具体信息}\\
    \hline
    操作系统 & Ubuntu 22.04 LTS\\
	CPU & AMD EPYC 7763 64-Core Processor @ 3.17GHz\\
	GPU & NVIDIA A100-SXM4-80GB Specs $\times$ 8\\
    内存 & 2TB\\
    \hline
    \end{tabularx}
    }
\end{table}

\subsection{测试软硬件环境}

应用相关的测试工作硬件环境如表~\ref{tab:hardware_env3}~所示，软件环境如表~\ref{tab:software_env3}~所示。

\begin{table}[!htbp]
    \centering
    \caption{软件环境配置表}
    \label{tab:software_env3}
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \scalebox{0.9}{
    \begin{tabularx}{\textwidth}{c Y}
    \hline
    \textbf{软件名称} & \textbf{版本信息}\\
    \hline
    Python & 3.9\\
	Pytorch & 2.3.1\\
    CUDA & 1.12.1\\
    \hline
    \end{tabularx}
    }
\end{table}

\subsection{性能测试与优化}

本文在应用阶段与数字人直接相关的算法如表~\ref{tab:resource_usage}~所示。采用长为400帧的人物视频片段对系统内各算法情况进行测试。该表中的模型批次大小因需考虑实际部署时的计算资源，为本文相关算法部署时的向下兼容时所采用的实际大小，而非测试环境的上限大小。同时在后续的性能优化描述中，将各功能点以 ID 的形式简略表示。

\begin{table}[h]
    \centering
    \caption{各功能模块的计算资源占用情况}
    \label{tab:resource_usage}
    \begin{tabular}{clcccc}
        \hline
        ID & 功能名称 & \makecell{显存占用\\（GB）} & 批次 & \makecell{每样本耗时\\（毫秒）} & 设备 \\
        \hline
        1  & 动作重定向与插值 & -     & -   & 6      & CPU \\
        2  & AiOS 提取         & 8.4   & 4   & 557    & GPU \\
        3  & HaMeR 手部提取    & 12    & 48  & 546    & GPU \\
        4  & Faceverse 提取    & 0.98  & 1   & 550    & GPU \\
        5  & FaceFormer 生成面部关键点 & 7.9 & 156 & 1.58  & GPU \\
        6  & Dwpose 提取关键点 & 0.9   & 1   & 80     & GPU \\
        7  & Meidapipe 提取关键点 & -  & 1   & 88     & CPU \\
        8  & 本文数字人模型（未包含初始化） & 5.1 & 1   & 27.7   & GPU \\
        9  & 本文数字人模型初始化过程 & 5.1 & -   & 9873  & GPU \\
        10 & 神经语义图像绘制  & 0.045 & 1   & 4.73   & GPU \\
        11 & MANO 手部图像绘制 & 0.01  & 1   & 350    & GPU \\
        12 & SMPLX 图像绘制(语义、法线) & 0.254 & 1 & 925 & GPU \\
        13 & 关键点数据对齐（面部、身体等） & -  & -   & 0.1    & CPU \\
        \hline
    \end{tabular}
\end{table}

\textbf{1)参数预存优化}

基于原始单目视频数据提取各人体先验模型并保存参数由功能 2、3、4、6、7组成，具有最大的时间开销，其每帧提取总时长接近 2 秒。因此即使在生成速度要求不强的节目制作数字人当中，如果在每次进行节目制作时，选择不同的人物片段进行动作编排时，就需要针对选择的视频片段重新提取相应的多维度数据集合，该延迟也是不可接受的，会极大的延长整个广电节目的制播周期。
因此在节目制播数字人系统中采用预先存取对应的视频文件的方式，所有的视频数据存入系统时均直接采用多维度特征提取数据集工具进行预处理，解耦视频数据与对应的先验模型提取流程。通过该方式，当用户实际使用系统时，选择对应视频片段实际为通过对应视频帧号检索出预存的先验模型参数，再进行后续的数字人动作编排工作。

在直播数字人应用中离线动作建模中同样是基于参数化的 SMPLX 模型进行的，同样使用该优化方式进行处理。

\textbf{2)直播数字人实时响应优化}

针对直播数字人，其具有严格的等待返回时间限制，正常情况下，用户期望的响应时间小于 2 秒。除了本文上述介绍的基于固定话术内容的预存策略，针对多维度条件图像中非神经语义图像的绘制（功能 11、12 组成，耗时约 1.3 秒），其仍然需要大量的绘制时间。因此针对离线动作建模，本文采用预存储多维度条件图像的方式，具体来说，针对动作状态机中相连的两个动作片段 $ A $与$ B $，对两个片段的首尾分别进行插值，分别得到两个过渡片段 $A_B$ 和 $B_A$，该过渡片段连接了动作状态机中的两个状态。以上动作片段构成了完整的$ A $ 至 $ B $的回路。此时对系统内的所有相连的动作进行上述过程，就能得到对应状态机内所有动作对应的先验模型参数，并绘制对应的多维度驱动图像预存。因此只要在实时生成过程中通过读取内存中预存的驱动图像，再结合生成的对应嘴型的神经语义图像，就可以得到完整的多维度合成图像序列。

而神经语义图像需要结合功能 5、10、13 进行实时的生成，这些步骤总耗时不超过 10 毫秒，可以认为能够达成实时响应的目的，因此保留在原始的视听联动功能中，可不进行改变。

此外，如表所示~\ref{tab:resource_usage}~第七列所示，本文数字人模型初始化过程需要消耗大量的时间，因此针对生成速度要求高的直播数字人，本文在加载状态机中动作数据的同时，同样预先加载模型参数，将模型参数存入显存，并配合使用 Torch Compile 和空向量推理等模型冷启动策略进行优化。当模型被调用时能直接加载显存中的参数，从而降低了模型每次响应推理的时间。

因此，本文将基于直播数字人的推理流程封装为类的形式，在类的初始化中进行图像序列初始化和模型初始化，随后作为服务端的方式接收请求并进行实时响应。该种实时响应优化本质上是以空间换时间的方式利用服务运行时更高的内存消耗换取更快速的响应。

\textbf{3)图像绘制优化}

在节目制作数字人中，需要对数字人动作进行更复杂在线编排，因此无法像直播数字人中依照动作状态机使用预存图像的方法。因此针对图像绘制，即功能 11、12 的耗时部分（1.3 秒），本文采用多进程绘制的方式进行优化。具体来说，当用户使用数字人节目制作应用时，服务器端会开辟一块进程间共享内存区域作为数组，并初始化对应的进程池，进程池中的每个进程首先会初始化绘制环境。当用户完成动作编排，进行确认后，经过插值和重定向的人体参数化片段会分配给进程池中的进程进行绘制，每个进程负责绘制其中一帧，当所有进程都完成绘制后，将绘制结果按照帧号存入共享内存，直到最后一个任务进程返回后，结束主进程等待并将共享内存中的数组结果返回给用户。在该架构下，可开辟的多进程数量由服务器资源决定，可进一步根据实际情况进行调整。

% \textbf{3)节目制作数字人的优化}


\section{本章小结}

在本章中，首先阐明了本文数字人模型推理流程，如何与其他先进技术集成以满足当前数字人应用需求。并以实际的两种数字人下游应用中的需求出发，分析应用底层围绕数字人所需实现的技术架构。通过实际应用案例中用户与系统的交互流程分析系统在交互过程中整体的数据通路。
最后在应用测试与优化部分，对本文实际部署时应用的所需的计算资源、时间性能进行测评，并探讨系统优化方案，闭环验证了本文提出的数字人建模方案在下游应用中的可行性。

