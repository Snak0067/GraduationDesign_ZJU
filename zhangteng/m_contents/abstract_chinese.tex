% 定义中文摘要和关键字
\begin{cabstract}
    目前，图结构数据广泛存在于社交网络、引文网络与电商推荐等真实场景中，其节点通常附带丰富的文本属性，构成文本属性图（Text-Attributed Graphs, TAGs）。当前，图大语言模型（Graph-LLMs）已成为处理这类数据的核心技术，在学术引文分析、电商关联推荐、金融风险挖掘等领域展现出实用价值。
    
    图大语言模型核心是融合图神经网络（GNN）的结构建模能力与大语言模型（LLMs）的语义理解优势，通过图序列化转换、跨模态表征投影等方式，实现图拓扑关系与文本语义信息的联合处理。尽管大语言模型在自然语言处理任务中展现出强大能力，但其在图结构理解与推理方面仍面临显著挑战。现有 Graph-LLMs 普遍存在图结构编码信息损失、图文表征空间割裂、依赖高质量人工标注数据以及推理过程不可追溯等问题，严重制约了模型在复杂图任务中的泛化能力与实用性。
    因此，为了解决上述的问题和挑战，本文提出一种结合强化学习的图结构大语言模型关键技术框架，主要工作包括：

    1)图结构表征改进：针对传统图模型在序列化过程中存在结构信息损失的问题，本文引入基于欧拉路径的可逆图序列化方法，通过引入节点身份多重编码与可重采样机制，实现图结构的无损、可逆序列化，有效保留全局拓扑信息，为模型统一处理节点、边与图级任务奠定基础。实验证明，该方法在OGB-PCQM4Mv2与ogbl-ppa数据集上较X基线提升约X\%。

    2)图 - 文跨模态表征的多层次对齐改进。为弥合图嵌入与文本语义空间的模态鸿沟，设计多层次跨模态对齐策略，融合全局对比损失、生成式交叉熵损失与细粒度匹配损失，分别从全局语义相似性、局部 token 级对应性、语义生成完整性维度，弥合图结构拓扑特征与文本语义特征的异构鸿沟，将跨模态相似度提升至 0.65 以上，显著提升图结构与文本语义在嵌入空间的一致性。

    3)基准训练范式与强化学习优化。为增强模型在复杂图推理场景下的泛化能力，结合HPT统一训练框架，将监督微调与强化学习融合，在无需人工偏好标注的前提下，引导模型生成逻辑正确、可验证的推理链。实验结果表明，所提方法在Erdős图论推理基准及Cora、PubMed、OGB-Arxiv等真实图数据集上均取得显著性能提升，不仅增强了模型的零样本泛化能力，还实现了可追溯、高可信的图推理过程。

    最后，本文整合上述技术方案，设计并实现了一套统一图语言模型原型系统，整合集成了节点分类和链路预测等下游任务在内的相关技术方案，通过系统功能和性能测试，验证了所提出方案的有效性和实用性。
\end{cabstract}
    
\ckeywords{图大语言模型，文本属性图，跨模态对齐，强化学习}