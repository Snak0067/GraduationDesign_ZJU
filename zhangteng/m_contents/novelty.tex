\chapter{帧间连续性与法线属性融合的时序一致性改进}

在上一章节中，本文设计并实现了一种多维度特征融合的条件式生成对抗基准模型。
通过引入人体大模型表征和局部图像位置判别器进一步在神经网络模型中融合了单帧图像中的全局和局部特征，提高模型的数字人建模质量。
然而，该基准模型属于单帧的图像生成模型，在数字人的视频生成任务当中，生成视频的时序一致性直接影响了数字人使用者的观看体验。

因此，本章提出了两种改进方案，分别基于帧间连续特征和嵌入法线特征进行多维度特征融合来改善模型生成数字人视频帧间的时序一致性。
基于帧间连续特征改进通过构建一个基于多帧的条件分布，将本文的基准模型由单帧条件建模拓展多帧条件建模，
期望通过构建帧间连续特征的条件分布能更好的建模数字人视频中的时序一致性。
嵌入法线特征的多维度融合是在现有 UNet 型生成器的解码器部分，构建一个额外的法线解码器分支，
该解码分支利用 Sapiens \cite{khirodkar2025sapiens}提取的人物法线图进行监督学习，
并进一步利用带有残差连接的卷积特征融合模块解码生成的法线特征图。

\section{概述}

从现实三维世界中采样得到的特定视频帧是二维的非结构化数据，在采样的过程中，其失去了连续帧之间的时序信息和原始空间下的三维结构信息。
因此当一个数字人视频生成模型仅通过单帧条件分布建模真实视频帧时，其无法保证生成视频帧前后的时序一致性，而时序一致性差可能表现为伪影、画面色彩抖动和数字人动作不自然，不规则运动等问题，均严重影响了模型生成的数字人视频的用户观看体验。

因此本节基于视频信息中的时序信息和三维结构信息，分别提出了基于帧间连续特征的时序一致改进和嵌入法线特征多维度特征融合。

\subsection{基于帧间连续特征的时序一致改进}

Vid2vid\cite{vid2vid} 中利用马尔可夫假设来对视频生成任务帧间连续性进行建模，
认为模型当前生成的帧 $\widetilde{Y}_t$ 仅与前 $L$ 帧的信息相关，$N$ 为原始视频总长度，
建模如公式（\ref{eq:vid2vid}）所示的视频条件分布:
\begin{equation}
    p(\widetilde{Y}_{1:N} \mid I_{1:N}) = \sum_{t=1}^N p(\widetilde{Y}_t \mid \widetilde{Y}_{t-L:t-1}, I_{t-L:t})
    \label{eq:vid2vid}
\end{equation}

Vid2vid 方法采用了自回归生成的策略进行视频图像建模，对于生成视频序列为 $\widetilde{Y}_{1:N}={\widetilde{Y}_1,\dots,\widetilde{Y}_N}$，其输入的条件图像序列为 $I_{1:N}={I_1,\dots,I_N}$。对于其中特定的一帧视频 $\widetilde{Y}_{t}$ 由序列数据 $\widetilde{Y}_{t-L:t-1}, I_{t-L:t}$ 共同驱动生成。
模型的目标是建模真实视频的分布，即令 $p(\widetilde{Y}_{1:N} \mid I_{1:N}) = p(Y_{1:N} \mid I_{1:N})$，其中 $Y_{1:T}={Y_1,\dots,Y_T}$ 为真实帧视频序列。

说话数字人嘴型编辑\cite{han2024text}当中探索了一种简单有效的生成约束方式。对于一段需要编辑原视频 $Y$，其中需要通过新的音频进行头像编辑的视频帧为长为 $T$ 的片段 $\hat{Y}$，在该方法中除在扩散模型当中注入驱动的音频模态之外，对该段视频的首尾帧 $\hat{Y}_{0}$ 和 $\hat{Y}_T$ 提取的关键点进行线性插值作为额外的辅助模态输入模型。

受上述思路的启发，本文将当前模型从单帧合成图像驱动拓展到多帧合成图像驱动。
假设在数字人视频生成过程中每帧的实际动作和表情为 $A_t$，其所在的连续帧片段为长度为 $2l+1$，则动作区间为 $A_{t-l:t+l}$。
当 $l$ 取值越小时，区间内的动作与当前帧动作 $A_t$ 的差异越小，关联性越强。因此可以认为在一个足够小的区间范围内，相邻多维度合成图像帧能够作为提示信息驱动当前帧 $\widetilde{Y}_t$，因此本文假设公式（\ref{eq:presudo}）为第 $t$ 帧生成数字人视频帧的分布。在本文中数字人动作 $A_t$ 由多维度合成图像 $I_t$ 表示。

\begin{equation}
    p(\widetilde{Y}_t \mid I_{t-l:t+l}) =  p(\widetilde{Y}_t \mid \sum_{i=t-l}^{t+l} \lambda_i I_i)
    \label{eq:presudo}
\end{equation}

当前的数字人生成帧 $\widetilde{Y}_t$ 可由一个在时序上长度为 $2l+1$ 的滑动窗口内的多维度合成图像进行建模。针对滑动窗口内的每一帧的多维度合成图像，均通过特定的权重 $\lambda_i$ 进行加权，该权重建模了第 $i$ 帧多维度合成图像与当前视频生成帧的关联关系，本文中利由神经网络学习得到该权重值。此外，如果从数据源的角度考虑该建模方式，其利用上下帧间的动作信息提示也有利于遏制驱动数据源本身提取时产生的噪音抖动，从而提高生成视频帧的时序一致性。

通过该分布假设，基于多帧连续性的科学问题描述为：

\textbf{输入：}用以驱动人物表情及动作的N帧的多维度合成图像序列$I_{1:N}=\{I_1, \ldots, I_N\}$，其中该序列包含:
\begin{enumerate}
    \item 手部图像序列 $I^{mano}_{1:N}=\{I^{mano}_{1}, \ldots, I^{mano}_{N}\}$
    \item 身体法线图像序列 $I^{normal}_{1:N}= \{I^{normal}_{1}, \ldots, I^{normal}_{N}\}$
    \item 身体语义图像序列 $I^{semantic}_{1:N}=\{I^{semantic}_{1}, \ldots, I^{semantic}_{N}\}$
    \item 神经语义图像序列 $I^{ccbr}_{1:N}=\{I^{ccbr}_{1}, \ldots, I^{ccbr}_{N}\}$
    \item 眼部注视图像序列 $I^{eye}_{1:N}=\{I^{eye}_{1}, \ldots, I^{eye}_{N}\}$
\end{enumerate}

任一合成模态图像子序列由空间大小一致的三通道连续 $2l+1$ 帧图像构成，即 $X_i \in \mathbb{R}^{F \times H \times W \times 3}$，其中H和W分别表示为驱动帧帧的长宽。对于特定的第 $t$ 帧建模时，子序列范围为 $X_{t-l:t+l}$。将以上五种模态在通道维度上进行拼接，即 $I_i \in \mathbb{R}^{F \times H \times W \times 15}$；
同时还输入用以建模不同帧之间人物细节变化的时序变量序列 $t_{1:N}=\{t_1, \ldots, t_N\}$，其中 $t_i \in \mathbb{R}^{64}$。

\textbf{输出：}模型根据多维度合成图像序列驱动生成对应数字人图像序列。$N$ 帧的图像序列表示为$\widetilde{Y}_{1:N}=\{\widetilde{Y}_1, \ldots, \widetilde{Y}_N\}$，其中$\widetilde{Y}_i \in \mathbb{R}^{{H}\times{W}\times{3}}$，其中H和W分别为视频帧的长宽。

\subsection{嵌入法线特征的多维度融合}

物体表面的法线指的是垂直于物体特定表面的向量，对于一个给定的表面点，法线表示即为该点的垂线。在图形学当中，法线的方向对于渲染和光照计算至关重要，物体表面的法线直接影响到光线如何与该表面进行互动，从而影响物体的视觉外观。同时图形学当中又可以单独定义法线贴图，在光栅化渲染当中用于模拟物体表面的细节，改变表面与光源之间的角度，从而影响物体的光照计算。单从法线贴图的视角来看，上一章节中所提出的基准模型结构也可以被理解为是根据多维度合成图像解码了数字人表面的纹理贴图，因此本文提出的模型同样能针对法线贴图进行解码。

此外在现实生活中物体的表面法线实际上表征了物体表面的相对几何结构。在当前从图片重建三维网格模型的研究当中，往往将法特征作为一个重要的约束项来提高神经网络模型的几何感知能力。如在 SF3D\cite{boss2024sf3d} 中使用了一个单独的法线解码器从三平面表示中解码法线与渲染出的网格法线真值间进行监督学习；wonder3D\cite{wonder3d} 中则采用了先重建模型法线再重建网格模型几何的方式。Instant Mesh\cite{xu2024instantmesh}则直接利用法线损失监督三维网格模型的几何结构和纹理细节，提高神经网络模型重建三维网格模型的精度和真实度。

因此本文认为，这种基于物体表面法线信息对模型生成结果的约束方法，能够使得模型中间层形成一定的三维感知能力，进而保持不同帧中数字人重建结果的一致性以提高模型的时序一致性。引入法线后，其科学问题定义如下：
% 该方法也是从几何建模的角度对于上一章节中基于判别器和损失从分布约束模型像素生成的有利补充。因此

\textbf{输入：}用以驱动人物表情及动作的N帧的多维度合成图像序列 $I_{1:N}=\{I_1, \ldots, I_N\}$。其中该序列包含:
\begin{enumerate}
    \item 手部图像序列 $I^{mano}_{1:N}=\{I^{mano}_{1}, \ldots, I^{mano}_{N}\}$
    \item 身体法线图像序列 $I^{normal}_{1:N}= \{I^{normal}_{1}, \ldots, I^{normal}_{N}\}$
    \item 身体语义图像序列 $I^{semantic}_{1:N}=\{I^{semantic}_{1}, \ldots, I^{semantic}_{N}\}$
    \item 神经语义图像序列 $I^{ccbr}_{1:N}=\{I^{ccbr}_{1}, \ldots, I^{ccbr}_{N}\}$
    \item 眼部注视图像序列 $I^{eye}_{1:N}=\{I^{eye}_{1}, \ldots, I^{eye}_{N}\}$
\end{enumerate}

其中任一合成模态由空间大小一致的三通道图像构成，即 $X_i \in \mathbb{R}^{H \times W \times 3}$，H和W分别表示为驱动帧帧的长宽。将以上五种模态在通道维度上进行拼接，即 $I_i \in \mathbb{R}^{H \times W \times 15}$；
同时还输入用以建模不同帧之间人物细节变化的时序变量序列 $t_{1:N}=\{t_1, \ldots, t_N\}$，其中 $t_i \in \mathbb{R}^{64}$。

\textbf{输出：}模型根据多维度合成图像序列驱动生成对应数字人图像序列。$N$ 帧的图像序列表示为 $\widetilde{Y}_{1:N}=\{\widetilde{Y}_1, \ldots, \widetilde{Y}_N\}$，其中 $\widetilde{Y}_i \in \mathbb{R}^{{H}\times{W}\times{3}}$，H 和 W 分别为视频帧的长宽。$N$ 帧的法线图表示为 $\widetilde{V}_{1:N}=\{\widetilde{V}_1, \ldots, \widetilde{V}_N\}$，其中 $\widetilde{V}_i \in \mathbb{R}^{{H}\times{W}\times{3}}$，H 和 W 与建模的视频帧长宽一致。

在 4.2 节中，详细介绍了基于三维分组卷积的时序特征融合方法，首先引入三维分组卷积模块建模各模态内的跨帧几何一致性，
并实现了一种数据增强策略，使得该三维分组卷积能够更加有效的利用前后帧信息。
%同时仍然将不同模态间融合的任务交予生成器主干网络完成。同时该模块轻量化的设计，几乎不影响模型的生成速度和计算量。

在 4.3 节中，详细介绍了基于多任务学习的法线特征融合方法，通过法线约束提高模型的时序一致性。在生成器原始图像解码分支外构建额外的法线解码分支，并利用残差连接的卷积特征融合模块将解码生成的法线特征图与原始图像特征图进行融合，提升了模型的几何感知能力，进而提高不同帧间人物外观的一致性。

% 在 4.4 节的实验中依靠第二章中提出的基准数据集构建方法，设计了一种验证数字人生成视频连续性的闭环检测方法。通过在实验室自采集数据集上的验证，证明了本文所提出的帧间连续性方法和法线多任务学习方法均对模型在时序一致性有正面的影响，能降低模型产生不期望的肢体抖动和不自然的跳变。

\section{基于三维分组卷积的帧间连续特征融合方法}

基于问题描述中的定义，本文使用基于分组三维卷积网络的时序融合层对上文提出的分布（\ref{eq:presudo}）进行建模，时序融合层对当前条件图像输入 $I_t$ 上下文的共 $2l+1$ 帧图像进行融合。具体来说，使用$\theta$表示该时序融合层，如式（\ref{eq:presudo_conv}）所示：
\begin{equation}
    \theta(I_{t-l:t+l}) \thickapprox  \sum_{i=t-l}^{t+l} \lambda_i I_i
    \label{eq:presudo_conv}
\end{equation}

针对每个独立的合成图像模态$X_i$，分组卷积其作为单独一组，在每组内仅针对该模态单独进行时序融合。
时序融合层使用的卷积核大小为 $(2l+1)\times1 \times1$，与帧序列长度一致，即仅进行时序上的降维和融合。当完成卷积计算后，分组卷积才将不同的合成图像模态在通道维度拼接。此时可表示为 $\hat{I}_t = \theta(I_{t-l:t+l})$，$\hat{I}_t$ 包含有通过卷积融合的前后多帧信息。与第三章的输入类似，$\hat{I}_t$ 被直接输入生成器主干网络中，以建模 $p(\widetilde{Y}_t \mid \hat{I}_t)$。本文在时序融合层使用分组卷积而不使用普通卷积层的目的在于，该结构专注于建模每个合成图像模态内部的时序信息，而 UNet 主干网络中的编码器层负责不同合成图像间的信息交互任务。同时分组卷积也进一步减少了时序融合层的参数规模，减少时序层带来的额外计算开销。

本文基于时序连续性方法在具体实现时，取 $l=1$ 作为序列长度。即使用 3 帧的滑动窗口大小，此时卷积核大小为$3\times 1 \times 1$，其时序融合层具体结构如图 \ref{fig:3dconv} 所示。为了进一步增强本文提出的帧间连续特征时序一致性策略中对滑动窗口内的前后帧对当前生成帧的约束作用，避免时序融合层只关注于当前与真实视频人物帧 $Y_i$ 的帧序号对应的合成图像驱动帧 $I_i$。本文参考realisdance\cite{zhou2024realisdance}中的时序增强策略，设计了基于时序特征的数据增强策略。数字人模型建模时，对于每一帧输入 $t$，确定一个长度为 $n$ 的数据增强序列区间 $I_{t-n:t+n}$，对于每次输入的时序驱动数据 $I_{t-l:t+l,l=3}=\{I_{t-1},I_{t},I_{t+1}\}$，将中间对应的合成图像帧 $I_{t}$ 以概率 $p$ 替换为数据增强序列区间 $I_{t-n:t+n}$ 中任意一帧其他合成图像帧数据。使用该时序增强策略等价于当中间对应的合成图像驱动帧 $I_t$ 的动作表情与真实图像视频帧 $Y_t$ 不一致时，时序融合模块通过学习上下文信息 $I_{i-t},I_{i+t}$ 来对当前帧结果进行推导。此外长度为 $n$ 的数据增强序列区间中替换帧应不与当前真实视频人物帧 $Y_i$ 的产生较大的动作和位置偏移，因此在实验中，取一秒的序列长度，将 $n$ 设置为 25 帧。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{m_figures/chapter5/时序融合层.pdf}
    \caption{三维分组卷积模块}
    \label{fig:3dconv}
\end{figure}

\section{基于多任务学习的法线特征融合方法}

在法线生成任务当中，首要解决的就是如何提取原始的人物表面法线作为监督信息。本文的数据集是基于真实场景下的单目视频构建，因此无法像三维合成数据集或基于 Lightstage 装置构建的扫描数据集，如zju-mocap\cite{peng2021neural}中直接获取真实的人物表面法线。因此本文利用预训练人体大模型 Sapiens\cite{khirodkar2025sapiens} 来提取人体的表面法线作为模型学习的伪真值。

具体来说，Sapiens 通过在大规模人体图像数据集上利用 MAE 方法\cite{he2022masked}训练得到了能够有效提取人物表征信息的 ViT 编码器\cite{dosovitskiy2021an}。Sapiens 方法中进一步构建由 600 个高分辨率摄影测量人体扫描和 HDRI 环境贴图组成的 50 万规模带有真实法线标注的合成数据集。Sapines 在 ViT 编码器后，连接一个由卷积和反卷积层组成的法线解码器，将模型微调到法线解码任务，最终使得模型能有效从人物图像数据生成对应人体表面法线。
在 Sapiens 中其法线估计的损失函数如公式（\ref{eq:loss_normal}）所示：
\begin{equation}
    \mathcal{L}_{\text{normal}} =  \sum_i \mathbf{M}_i \| \mathbf{n} - \hat{\mathbf{n}} \|_1 +  \sum_i \mathbf{M}_i (1 - \mathbf{n} \cdot \hat{\mathbf{n}})
    \label{eq:loss_normal}
\end{equation}

其中 $\mathbf{n}$ 法向量真值，$\hat{\mathbf{n}}$ 为模型预估的法向量。其分别使用L1损失和余弦损失作为法线解码任务的损失函数。此外，Sapiens 使用微调的掩码解码器输出掩码 $M$ 使得模型仅对人物区域进行法线预测和损失计算。对于人体表面任意一点的法线向量，其被归一化到三维空间下 $x^2+y^2+z^2=1$ 的球面上。

本文的法线损失参照如上损失函数定义。但与之不同的是，Sapiens 法线提取时输入的是一张人物图片，模型输出对应的人物表面法向量。因此在 Sapiens 的推理流程中总是可以利用对应的 Sapiens 掩码解码器对非人物区域进行掩码，从而忽略模型针对非人物区域的随机输出。而本文中法线解码任务与前文所述视频数字人生成一致，需要直接建模从多维度合成图像到人物法线图像的映射。因此要求模型在非人物区域输出趋近于0，而在人物区域接近Sapiens的输出。

因此本文直接与 Sapines 输出的带掩码的法线图（掩码区域值被置为0）进行法线损失计算。通过直接的 L1 监督使得在掩码区域模型的输出趋近于 0，在非掩码区域接近于真实法线。通过引入基准数据集构建的 SAM2 掩码 $M$ ，使得余弦损失计算仅应用于人体区域，而被定义为零向量缺少模长的非人物区域，通过掩码避免余弦损失计算。

上一章中，本文已经介绍过本文基于 StyleUNet\cite{styleAvatar} 的生成器骨干网络中编码器和解码器结构。在本节中，为了能够重建对应帧的人物表面法线，需要对原始的解码器结构进行修改，在其中构建一个具有法线解码器的分支网络，以多任务学习的方式在模型中引入法线约束。
与图像解码器类似，通过法线解码器需要输出法线图像 $\widetilde{V}$，利用损失（\ref{eq:loss_normal}）与 Sapiens 中提取的伪法线真值进行重建损失计算，该分支将梯度传回与图像解码器共同的编码器部分进行联合训练。

解码器中加入该分支网络后的结构如图~\ref{fig:normal_decoder}~所示。其中来自上一层的法线图像输出为 $\widetilde{V}_{d-1}$，$\widetilde{V}_{d}$ 表示当前层输出的法线图像。法线解码器与图像解码器基本结构一致，但不使用调制卷积层，而是更换为普通的卷积层，并移除了图像解码器中的逐通道偏置量 $b_d$。前者将调制卷积层替换为普通卷积层是因为法线表征的物体表面几何结构信息相较于表面纹理信息相对简单，同时在多帧之间不应受到人物局部细节，如光照变化等的影响，因此不需要在解码器部分进一步与时序风格向量$\mathcal{W}$ 做进一步的调制。而移除通道偏置是因为期望法线解码器输出的通道维度表示的是三维空间下法向量的 $(x,y,z)$ 取值，与 RGB 图像相比不存在整体的偏移量。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chapter5/法线多任务分支.pdf}
	\caption{多任务解码器结构图}
	\label{fig:normal_decoder}
\end{figure}

法线解码器将当前层风格卷积层的输出进一步通过卷积层解码成相应的特征信息；通过跳跃连接将来自上一层的 $\widetilde{V}_{d-1}$ 上采样后与特征信息进行相加，最终得到当前层的输出法线图 $\widetilde{V}_{d}$。

基于法线分支解码器的设计本质上是采用多任务学习的方式，将重建人物表面法线作为训练时提高模型编码器几何表征能力的代理任务。在该框架下，单纯使用法线解码器其生成的法线图并未在模型推理驱动阶段得到更好的利用。同时在本文当前的数字人生成框架下，模型直接通过隐含的参数空间直接对驱动条件进行分布映射，无法使用类似光栅化的图形学渲染方式，将法线图直接作为法线贴图去优化模型的光照结果。

因此为了能够利用重建得到的法线图，本文利用神经网络隐式建模的方式将法线图特征 $\widetilde{V}$ 作为额外的驱动条件注入图像解码器当中。
具体来说，本文设计了基于卷积神经网络和残差连接结构的法线融合层，法线融合层用以融合模型风格卷积层的隐含特征和法线解码器输出的法线特征，其基本结构如图 \ref{fig:normal_fuse} 所示。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chapter5/法线融合单元.pdf}
	\caption{法线融合单元结构图}
	\label{fig:normal_fuse}
\end{figure}

其中风格卷积层输出的特征与法线解码器输出的法线 $\widetilde{V_d}$ 在法线融合层首先进行通道维度拼接，再通过卷积层对其特征进行混合。该模块通过跳跃连接的方式，将法线融合层中经过混合的特征与风格卷积层输出的特征直接相加，将该特征作为图像解码器新的输入特征。本文设计的法线融合层结构在能够有效混合法线特征的同时通过跳跃连接的方式有利于保持原始风格卷积层对图像解码器的驱动能力。通过引入法线融合层，本文将法线特征作为额外的多维度条件注入到图像解码器当中，从而使得模型能够融合法线特征。

在本章的实验部分，将证明法线解码器的结构设计能够完成法线重建的任务。模型通过法线多任务学习的方式提升了模型编码器部分的几何感知能力，最终达成提升模型生成的数字人的时序一致性的目的，并期望能够提高生成效果。其次，本文横向对比了仅使用法线分支解码器进行法线重建与完整使用法线融合模块进行法线特征融合的模型生成效果，证明该策略能够进一步提升模型的生成质量，进而说明法线特征融合层的优越性。

\section{模型测评与实验分析}
\subsection{实验目标与配置}
\textbf{1)实验目标}

本章提出的嵌入法线特征的多维度融合和基于帧间连续特征的时序融合方法均是针对数字人生成中的时序一致性问题提出的改进。本实验中首先需要验证改进模块的有效性，同时针对时序一致性改良结果进行进一步验证。同时本文提出的方法需要与现有开源方法进行横向比较，进一步验证本文多维度特征融合建模的数字人性能。

因此本章重点的实验目标有以下三点：
\begin{enumerate}
    \item 验证本节提出的各模块的有效性。
    \item 验证本节提出的改进能够提高模型生成的数字人时序一致性。
    \item 横向对比现有开源模型，验证多维度特征融合建模的数字人性能。
\end{enumerate}

围绕上述目标，本节设计了三组实验分别对以上的三点目标展开实验，对本节提出方案进行详细的评估。

\textbf{2)环境配置}

本方案相关的训练和测试工作的硬件环境如表~\ref{tab:hardware_env2}~所示，软件环境如表~\ref{tab:software_env2}~所示。

\begin{table}[!htbp]
    \centering
    \caption{硬件环境配置表}
    \label{tab:hardware_env2}
    \scalebox{0.9}{ 
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\textwidth}{c Y}
    \hline
    \textbf{硬件名称} & \textbf{具体信息}\\
    \hline
    操作系统 & Ubuntu 22.04 LTS\\
	CPU & AMD EPYC 7763 64-Core Processor @ 3.17GHz\\
	GPU & NVIDIA A100-SXM4-80GB Specs $\times$ 8\\
    内存 & 2TB\\
    \hline
    \end{tabularx}
    }
\end{table}

\begin{table}[!htbp]
    \centering
    \caption{软件环境配置表}
    \label{tab:software_env2}
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \scalebox{0.9}{
    \begin{tabularx}{\textwidth}{c Y}
    \hline
    \textbf{软件名称} & \textbf{版本信息}\\
    \hline
    Python & 3.9\\
	Pytorch & 2.3.1\\
    CUDA & 1.12.1\\
    \hline
    \end{tabularx}
    }
\end{table}

\textbf{3)数据集配置}

本章的实验主要在实验室自采集人物上进行。在横向对比实验中，对其他模型的测试和微调采用相同的训练测试样本切分。

\begin{table}[!htbp]
    \centering
    \caption{数据集配置表}
    \label{tab:dataset}
    \scalebox{0.9}{ 
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\textwidth}{c Y Y Y}
    \hline
    \textbf{人物形象} & \textbf{示例图片}& \textbf{训练集数量（帧）} & \textbf{测试集数量（帧）}  \\
    \hline
    自采集人物 & 
    \begin{minipage}[Y]{0.25\textwidth}
    \centering
    \includegraphics[width=\linewidth]{./m_figures/chapter4/result/spa0.6b/org.png}
    \end{minipage} & 17,500 & 1,000 \\
    \hline
    \end{tabularx}
    }

\end{table}

\textbf{4)实验细节}

\textbf{1)预训练策略} 

在本节中，本文通过预训练微调的策略，使用在第三章中预训练的模型，加速模块的验证过程。具体来说，基准模型采用的方案为：

\begin{enumerate}
    \item 基于 Sapiens 0.6B 参数量模型的特征损失。
    \item 基于局部关键位置判别器的局部特征融合策略。对于局部关键位置判别器的生成损失和 Sapiens 特征损失均采用 1 的权重。
    \item 模型采用 $1024 \times 1024$ 的分辨率尺寸，模型的数据批次大小为8，在自采集人物上进行了60轮的预训练。在预训练过程中，对于所有模型组件，采用2e-3的恒定学习率，并使用 Adam 优化器进行模型优化，并使用bfloat16混合精度训练。具体参数如表~\ref{tab:pretrain}~所示。
\end{enumerate}

\begin{table}[!htbp]
    \centering
    \caption{模型预训练的具体参数}
    \label{tab:pretrain}
    \scalebox{0.9}{ 
    \begin{tabular}{@{}ll@{}}
    \toprule
    \textbf{参数}             & \textbf{策略}                   \\ \midrule
    基准模型策略                 & Sapiens 0.6B 特征损失策略 \\
                             & 局部关键位置判别器策略 \\
    生成损失权重$\lambda_local$             & 1  \\
    特征损失权重$\lambda_f$             & 1           \\
    分辨率尺寸               & $1024 \times 1024$            \\
    数据批次                 & 8                             \\
    训练轮次               & 40 轮次                       \\
    学习率                   & 2e-3                          \\
    优化器                   & Adam                          \\
    混合精度             & bfloat16                      \\ \bottomrule
    \end{tabular}
    }
    \end{table}

\textbf{2)微调策略} 

针对微调阶段，基准模型不进行权重冻结，与新增加的模块结构进行协同训练。在本文中，分别在基准模型增加额外模块进行了四组配置训练。其中配置一，在基准模型中加入三维卷积时序融合模块；配置二，在基准模型中加入仅法线解码器结构；配置三，在基准模型中加入完整的法线融合单元；配置四，在基准模型中加入分别加入完整的法线融合单元与三维卷积时序融合模块。

在以上微调实验中，均采用2e-3的恒定学习率，并使用 Adam 优化器进行模型优化，使用 bfloat16 混合精度进行40轮次的训练。对于提出的帧间连续性数据增强策略，采用0.05的增强概率。对于第四组配置，其分别利用配置一预训练完成的三维卷积时序融合模块和配置三中预训练完成带有法线融合单元的主干网络，再进行20轮的融合训练。
% 具体参数如表\ref{tab:finetune}所示。
% \begin{table}[!htbp]
%     \centering
%     \label{tab:finetune}
% \caption{微调阶段实验参数}
%     \begin{tabular}{@{}lll@{}}
%     \toprule
%     \textbf{实验组} & \textbf{参数} & \textbf{策略} \\ \midrule
%     \multirow{6}{*}{时序融合模型微调} & 权重冻结 & 否 \\
%      & 额外模块 & 三维卷积时序融合模块 \\
%      & 时序增强策略概率$P$ & 0.05 \\
%      & 学习率 & 2e-3 \\
%      & 优化器 & Adam \\
%      & 混合精度训练 & bfloat16 \\
%      & 训练轮次 & 40 \\\midrule
%     \multirow{6}{*}{法线分支模型微调} 
%     & 权重冻结 & 否 \\
%     & 额外模块 & 法线解码器 \\
%      & 学习率 & 2e-3 \\
%      & 优化器 & Adam \\
%      & 混合精度训练 & bfloat16 \\
%      & 训练轮次 & 40 \\ \midrule
%     \multirow{6}{*}{法线融合模型微调} 
%     & 权重冻结 & 否 \\
%     & 额外模块 & 法线特征融合模块\\
%      & 学习率 & 2e-3 \\
%      & 优化器 & Adam \\
%      & 混合精度训练 & bfloat16 \\
%      & 训练轮次 & 40 \\ \midrule
%      \multirow{7}{*}{法线与时序融合模型微调} 
%      & 权重冻结 & 否 \\
%      & 额外模块 & 法线特征融合模块 \\
%       &           & 三维卷积时序融合模块 \\
%       & 学习率 & 2e-3 \\
%       & 优化器 & Adam \\
%       & 混合精度训练 & bfloat16 \\
%       & 训练轮次 & 20 \\ 
%      \bottomrule
%     \end{tabular}
%     \end{table}


% 对于所有的横向对比测评均在单张A100 GPU上进行，并在推理中使用实际可行的最大批次大小。

\subsection{模型结构消融实验}

1)\textbf{实验设计}

在该部分，本文针对第四章中基于帧间连续性和法线属性融合提出的神经网络模块进行结构消融实验，并进一步针对法线模块进行对比测评。
具体来说，将本文第三章中基于 Sapiens 0.6B 感知损失和多判别器局部增强方案基准模型称为 “Sapiens 0.6B”；仅法线解码模块加入基准模型的方案称为“法线分支”；将完整的法线融合模块加入基准模型的方案称为“法线融合”；将三维卷积时序融合模块应用于基准模型的方案称为“时序融合”；将法线融合模块与三维卷积时序融合模块均加入基准模型的方案称为“融合模型”；将融合模型额外训练60轮称为“融合模型 (额外训练)”。以上各方案模型使用相同的自采集人物测试集进行测试，获得相应的可视化结果和指标。


2)\textbf{实验结果}

表~\ref{tab:normal_diff}~定性比较了“法线分支”和“法线融合”方案在测试集中重建的法线与真值法线的平均余弦距离误差。

\begin{table}[!htbp]
    \centering
    \caption{法线生成余弦差异对比}
    \label{tab:normal_diff}
    \scalebox{0.9}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \textbf{法线方法} & \textbf{余弦差异$\downarrow$} \\ \midrule
    法线分支 & 0.1019 \\
    法线融合 & 0.1020 \\ \bottomrule
    \end{tabular}
    }
\end{table}

表~\ref{tab:novel_performance}~展示各实验方案在测试集中指标结果，其中加粗表示最佳结果，下划线表示排名第二的结果。

\begin{table}[h]
    \centering
    \caption{时序一致性模型性能比较}
    \label{tab:novel_performance}
    \scalebox{0.9}{
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        \textbf{Model} & \textbf{APD $\downarrow$} & \textbf{MAPD $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{PSNR $\uparrow$} & \textbf{SSIM $\uparrow$} & \textbf{FVD $\downarrow$} \\
        \midrule
        Sapiens 0.6B & 3.89 & 26.90 & 0.025 & 29.64 & 0.968 & 99.94 \\
        法线分支 & 4.42 & 23.97 & 0.025 & 30.06 & 0.968 & 55.45 \\
        法线融合 & \underline{3.55} & 21.07 & 0.021 & 30.79 & \underline{0.970} & 64.20 \\
        时序融合模型& 4.10 & 22.52 & \underline{0.020} & \underline{31.15} & \underline{0.970} & \underline{33.94} \\
        融合模型 & 3.82 & \underline{20.45} & \underline{0.020} & \underline{31.15} & \textbf{0.971} & \textbf{33.92} \\
        融合模型 (额外训练) & \textbf{3.55} & \textbf{19.76} & \textbf{0.018} & \textbf{31.73} & \textbf{0.971} & 36.10 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

3)\textbf{实验分析}

针对法线模块的不同设计，表~\ref{tab:normal_diff}~从指标上说明了“法线融合”和“法线分支”方案的法线解码器具有类似的法线重建能力，其中“法线融合”方案的余弦差异比“法线分支”方案高了千分之一。这种微小差异是因为“法线融合”方案通过构建了法线融合层将法线解码器输出的法线特征连接到图像解码器，因此在模型优化的过程，法线分支解码器会受到两组梯度的影响，通过图像解码器传导的梯度可能在一定程度上影响法线解码器的解码能力。

表~\ref{tab:normal_diff}~中则横向对比了“法线分支”和“法线融合”方案中图像解码器的图像生成质量。
“法线分支”方案中 MAPD 指标相较于基准方案“Sapines 0.6B”下降了 11\%，在 LPIPS、PSNR、SSIM 指标中分别优化16\%，3.9\%和0.2\%，FVD 指标则下降约 44.42\%，说明法线分支方法通过提高模型对图像几何细节的捕捉能力，从而在在时序一致性指标和像素级指标均得到提升。“法线融合”相较于“法线分支”方案指标进一步提升，其中 APD 指标下降 19.7\%,MAPD指标下降约 12.1\%，LPIPS、PSNR、SSIM 指标分别较基准方案“Sapines 0.6B”下降16\%，提高2.4\%和0.2\%，但FVD反而提高了 15.8\%。
因此从定性结果来看，“法线融合”相较于“法线分支”方案，在视频生成的时序一致性上稍弱，但在视频生成质量上更强；通过引入法线融合模块，将法线解码器输出的法线特征与其他特征进行融合，能够进一步增强模型对图像几何细节的理解能力。

针对三维卷积时序融合模块，表~\ref{tab:novel_performance}~表明“时序融合”方案在 FVD 指标的下降上最为明显，相较于基准方案“Sapines 0.6B”，其降低了约 66\%，在LPIPS、PSNR、SSIM 等多个指标中分别下降20\%，提高5.1\%和0.2\%，该定性指标证明通过引入三维卷积时序融合模块，能够有效捕捉视频序列中连续帧之间的时序一致性，并在一定程度上改良生成视觉质量。

“融合模型”同时引入法线融合模块和三维卷积时序融合模块，实现了多维度特征的综合利用。这种综合性特征融合策略在多个指标上均表现出显著的性能提升，尤其是在 MAPD、LPIPS 和 FVD 指标上相较于基准方案“Sapines 0.6B”分别降低了 24\%，20\%和 66\% 。这一结果表明，“融合模型”不仅能显著改善视频的时序一致性，还能够有效提升感知质量。此外，PSNR 和 SSIM 指标的进一步提升表明，“融合模型”在保持图像结构信息和细节方面也具有更强的能力。相比基准方案“Sapines 0.6B”，“融合模型”在整体性能上实现了全面的优化。

而“融合模型 (额外训练)”在除 FVD 所有指标上在“融合模型”的基础上进一步提高，在 APD、MAPD、LPIPS、PSNR 指标上分别得到了 3\%\~10\% 左右的改进，获得了最优的结果。说明本文提出的模型结构通过延长训练时间和训练样本量能得到更加充分的优化，进一步提升模型的建模效果和稳定性。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chapter5/result/法线图对比.drawio.pdf}
	\caption{法线图重建可视化效果对比}
	\label{fig:comparison_normal}
\end{figure}


4)\textbf{可视化结果与分析}

图~\ref{fig:comparison_normal}~中分别展示了原始视频帧、Sapiens 模型输出的法线图真值、和经过掩码的“法线分支”和“法线融合”方案中法线解码器输出。“法线融合”输出的法线图存在一些不合理的部分，出现类似于噪点的现象。而“法线分支”的输出整体较为平滑。
进一步针对“法线融合”中的输出图像进行分析，这类噪点易出现在牙齿，眼睛和身体边缘处。本文认为这种现象是因为“法线融合”中法线解码器得到的法线特征通过法线融合层连接到图像解码器，图像解码器中针对人物图像进行解码学习时，图像的边缘信息往往因为差异大，具有较大的梯度，该梯度经由法线融合层，反向传播回法线解码器，从而导致了法线噪点的产生。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chapter5/result/实验-法线对比.pdf}
	\caption{法线方案数字人图像可视化效果对比}
	\label{fig:comparison_normalRGB}
\end{figure}

图~\ref{fig:comparison_normalRGB}~中对比了“法线融合”方案与“法线分支”方案下图像解码器的输出结果，分别给出了原始视频帧、“法线分支”方案生成视频帧和与原始视频帧间的热力图，“法线融合”方案生成视频帧和与原始视频帧间的热力图。对应的生成视频帧与原始视频帧相比，“法线融合”方案在生成的整体数字人形象上具有更高的还原度，在热力图上表现为“法线分支”方案在身体边缘区域拟合存在明显差异（红色区域标识）。

此外，根据第二列的生成结果比较，虽然“法线融合”方法在该帧中与原始视频帧相比表面具有更多的轻微差异区域（绿色区域标识），但沿着图中横向观察“法线融合”方法的生成帧，其人物表面亮度基本统一，说明通过融合的法线特征在一定程度发挥了类似法线贴图的能力，从时序编码中解耦了人物光照，使得重建的数字人能够在时序上保持一致的亮度与色彩结果。结合表~\ref{tab:novel_performance}~中关于法线模块的定性分析结果，“法线融合”方案与“法线分支”方案相比能够进一步提高图像解码器的生成质量，当法线作为隐式特征使用时，不会对模型多维度特征融合能力造成影响，导致图像解码能力下降。

% 因此当梯度由图像解码器传回给法线解码器时，本文最终的目标是建模高质量的数字人，因此这种方式是利大于弊的，虽然在一定程度上破坏法线解码器本身的能力，但是有助于模型提高最终图像解码的能力。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{./m_figures/chapter5/result/与基准方法比较_new.pdf}
	\caption{改进模型与基准模型关键部位可视化效果对比}
	\label{fig:comparison_base}
\end{figure}

图~\ref{fig:comparison_base}~中则比较了本章“融合模型”方案与上一章最优模型“Sapiens 1B”方案模型在关键部位的生成效果。分别给出了神经语义图像、原始视频帧、“融合模型”生成视频帧和与原始视频帧间的热力图，“Sapiens 1B”生成视频帧和与原始视频帧间的热力图。

首先针对面部重建结果，二者与原始模型在表情上均存在一定的重建误差，二者生成的质量较为接近，从生成视频帧中直接观察，“融合模型”方案生成的表情会更接近原始视频帧。进一步根据生成结果热力图，“融合模型”方案在面部关键部位生成效果优于“Sapiens 1B”方案，这体现在面部存在轻微差异的区域更少（绿色区域标识）。但二者针对面部边缘区域拟合都存在明显差异（红色区域标识）。

而针对手部区域，“融合模型”和“Sapiens 1B”均能够忠实的由对应的多维度驱动图像进行驱动。直接观察生成视频帧中的手部纹理，“融合模型”方案相较“Sapiens 1B”方案更加清晰 。同时在热力图中也表现为具有更少的轻微差异区域（绿色区域标识）。

综上所述，本节定性与定量指标相吻合。本文提出的帧间连续性与法线属性融合方案，在时序一致性和视频质量方面均超过基准模型。证明了本节所提出的方案在数字人建模任务中的有效性。

\subsection{基于闭环检测的模型时序一致性测评}

1)\textbf{实验设计}

与其他视频生成任务不同，除了直接评估视频帧之间的的视频质量指标其是否得到了改善，需要提出针对于数字人任务的时序测评方法。本节中依托于第二章提出的基于多维度特征的数字人基准数据集构建方法实现数字人时序闭环检测方法来评估模型时序能力。

具体来说，针对一段驱动人物表情及动作的N帧的多维度合成图像序列 $I_{1:N}=\{I_1, \ldots, I_N\}$，其中 $I_{1:N}$ 由原始视频 $Y_{1:N}$ 利用基准数据集构建方法提取得到。对于基准数据集中间产生的所有多维度数据集合称为$MID_{1:N}$，该数据集合包含第二章数据集构建中所述各数据子项，如身体二维投影关键点，SMPLX 姿态系数等。

此时利用 $I_{1:N}$ 驱动模型对应的生成的 $N$ 帧的数字人视频序列表示为 $\widetilde{Y}_{1:N}=\{\widetilde{Y}_1, \ldots, \widetilde{Y}_N\}$，利用基准数据集构建方法，重新从生成数字人视频 $\widetilde{Y}_{1:N}$ 当中提取出对应的数字人多维度特征表示 $\widetilde{MID}_{1:N}$。为了从该多维度特征表示中计算对应数字人的时序一致性，利用 SMPLX\cite{smplx} 回归的身体关节点投影到二维像素平面，根据身体关节点在时序中的变化来衡量模型生成数字人结果的时序一致性，具体来说评估如下几个指标。
% 和SMPLX 根节点旋转
\begin{itemize}
    \item \textbf{平均位移}：衡量单个视频内逐帧之间人物所有关键点的平均移动距离。该指标说明了在整个序列上数字人动作变化的幅度。当数字人动作变化幅度越大，该指标越高。对于生成的数字人视频，该指标与原始提取对应指标越接近，说明数字人更符合原视频运动幅度。
    \item \textbf{相对平均位移}：衡量生成数字人视频与原始人物视频之间的逐帧所有关键点的平均距离。该指标直接说明模型在当前帧生成结果与原始人物视频之间的动作差异，该指标越小，说明当前动作与原始视频越接近。
    \item \textbf{最大位移}：衡量单个视频内的视频人物关键点帧间最大位移。该指标说明在一个视频序列中帧间人物最大的运动幅度。
    \item \textbf{相对最大位移}：衡量生成式数字人视频中与原始人物视频间关键点帧间最大位移。该指标说明模型在当前序列生成结果与原始人物视频之间最大的动作差异。该指标越小，说明当前动作与原始视频越接近。
\end{itemize}
% 对于根节点旋转，

2)\textbf{实验结果}

通过该闭环检测工具，本文得到如表~\ref{tab:displacement_performance_comparison}~所示的时序分析表。

\begin{table}[ht]
    \centering
    \caption{不同模型的时序分析}
    \label{tab:displacement_performance_comparison}
    \scalebox{0.9}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    方法 & 平均位移 & 最大位移 & 相对平均位移$\downarrow$ &  相对最大位移$\downarrow$ \\ \midrule
    原视频 & \textit{1.914}  & \textit{18.874} & 0  & 0 \\
    基准模型 & 1.956& 16.822 & 4.394 & 24.301 \\
    法线融合 & 1.933 & 15.610 & \underline{2.931} &  42.030 \\
    时序融合 & 1.897 & 17.338 & 3.779 &  \underline{20.399} \\
    融合模型 & 1.934  & 18.488 & \textbf{2.889} &  \textbf{17.051} \\ \bottomrule
    \end{tabular}
    }
    \end{table}
    

3)\textbf{实验分析}

仅考虑单一视频内的平均位移和最大位移，其中“时序融合”利用连续帧特征作为输入，其得到了最小的平均位移，在整个序列中具有最小的运动幅度，跨帧输入使得序列特征在一定程度得到平滑。而法线融合则具有最小的最大位移，其通过跨帧几何一致性约束了模型的运动范围。而在融合模型中，其平均位移和最大位移均表现出了与原视频近似的特点，说明法线特征融合与帧间连续特征对保持数字人原始运动具有共同促进作用。

考虑相对平均位移和相对最大位移，融合模型方案同样取得了最好的指标结果。与原始动作序列保持最高的一致性。而时序融合方案和法线融合方案分别在相对平均位移和相对最大位移指标上表现较好。
综合以上结果，本文提出的法线融合模块能有效的学习几何信息进而控制数字人的相对运动，而时序融合则通过多帧连续性提高了数字人生成结果的平滑性，均对模型的时序一致性产生了正面的作用。


\subsection{模型性能测评}
1)\textbf{实验设计}

该实验将本文提出的模型与其他现有模型进行横向对比测评，验证本文提出模型的性能指标。本文采用 LIA 方法\cite{wang2024lia}和 realisdance 方法\cite{zhou2024realisdance}对相同的测试样本进行测评。二者均为具有零样本泛化能力的数字人模型，二者分别在 GAN 方法和 Diffusion 方法中取得了零样本泛化的领先效果。该实验中，针对测试样本，从自采集人物中前序实验未采用视频额外划分了 2500 帧进行测试。

本节实验中首先直接使用原始论文提供的对应模型权重直接对测试样本进行推理，并进行指标测评。同时为了进一步公平比较其他方法的效果，本文在相同的训练样本中对以上两个模型在预训练模型的基础上进行微调，针对 realisdance 方法，本文参照原始论文方法，采用两阶段微调方法，第一个阶段使用条件图像、真实图像数据对的方式训练模型的动作引导网络、主干网络和人物参考网络；第二个阶段冻结其他模块，使用条件图像序列、真实图像序列数据对的方式微调模型的时序层。本文使用具体微调参数参照原始论文模型训练参数。具体模型微调参数如表~\ref{tab:othermodel_parm}~所示。

\begin{table}[!htbp]
    \centering
    \caption{横向对比模型实验参数}
    \label{tab:othermodel_parm}
    \begin{tabular}{@{}lll@{}}
    \toprule
    \textbf{实验组} & \textbf{参数} & \textbf{策略} \\ \midrule
    \multirow{6}{*}{LIA} 
    & 模型分辨率 & 256 $\times$ 256 \\
    & 初始化权重 & ted-talk预训练 \\
     & 学习率 & 2e-2 \\
     & 优化器 & Adam \\
     & 混合精度训练 & 未使用 \\
     & 训练轮次 & 20 轮次 \\ \midrule
    \multirow{8}{*}{realisdance} 
    & 训练模块 & diffusion主干网络 \\
    & 模型分辨率 & 768  $\times$ 576 \\
    & 初始化权重 & 混合数据集预训练 \\
     & 学习率 & 1e-5 \\
     & 优化器 & Adam \\
     & 混合精度训练 & float16 \\
     & 学习率周期 & 带 warmup 的恒定学习率 \\
     & 训练轮次 & 第一阶段 10 轮次、第二阶段 5 轮次 \\ \bottomrule
    \end{tabular}
    \end{table}

在该节中，将本文中法线融合模块与三维卷积时序融合模块加入基于 Sapiens 0.6B 基准模型后额外训练 60 轮的最终模型称为“本文模型”方案，将LIA微调前后分别称为“LIA”和“LIA（微调）”方案。将realisdance微调前后分别称为“realisdance”和“realisdance（微调）”方案，将本文第三章中提出的基于“Sapiens 0.6B”的基准模型方案称为“基准模型”，将本文中单独的三维分组卷积融合层方案称为“时序层”，将把单独的法线分支模块加入基准模型的方案称为“法线分支”。将完整的法线融合模块加入基准模型的方案称为“法线融合”。将三维卷积时序融合模块应用于基准模型的方案称为“时序融合”。

2)\textbf{实验结果}

表~\ref{tab:model_time_performance}~中横向对比了本节所有方案的时间性能、显卡占用和生成分辨率。在生成过程中，所有模型都使用大小为 1 的推理批次大小，特别的对于 realisedance 模型，其在前向传播时具有额外的帧维度 $f$，其每次输入的帧数为 14，并以一半帧数大小重叠的滑动窗口遍历数据。

\begin{table}[!htbp]
    \centering
    \caption{模型性能对比表}
    \label{tab:model_time_performance}
    \scalebox{0.9}{
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \textbf{模型方案} 
    & \makecell[c]{\textbf{平均单帧推理耗时}\\(毫秒)}
    & \makecell[c]{\textbf{内存占用大小}\\(GB)}
    & \makecell[c]{\textbf{生成分辨率}\\(像素)}
     \\  \midrule
    LIA\cite{wang2024lia} & 52 & 1.3 & $256 \times 256$ \\
    realisdance\cite{zhou2024realisdance}（DDIM 30步）& 9200 & 32 & $512 \times 768$ \\
    基准模型 & 18 & 3.7 & $ 1024 \times 1024 $ \\
    时序层 & 0.7 & 0.5 & - \\
    时序融合 & 18.7 & 4.2 & $ 1024 \times 1024 $ \\
    法线分支 & 25 & 3.7 & $1024 \times 1024 $ \\
    法线融合 & 27 & 4.6 & $1024 \times 1024 $ \\
    本文模型 & 27.7 & 5.1 & $1024 \times 1024 $ \\ \bottomrule
    \end{tabular}
    }
    \end{table}

表~\ref{tab:model_performance_comparison}~中比较了展示上述方案中的模型在实验室自采集人物2500帧测试集中LPIPS、PSNR、SSIM和FVD的指标结果，其中加粗表示最佳结果，下划线表示排名第二的结果。

\begin{table}[!htbp]
    \centering
    \caption{模型性能比较表}
    \label{tab:model_performance_comparison}
    \scalebox{0.9}{
    \begin{tabular}{@{}lcccc@{}}
    \toprule
    模型 & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & FVD$\downarrow$ \\ \midrule
    LIA & 0.079 & 20.02 & 0.865 & 359.13 \\
    LIA微调 & 0.036 & 19.70 & 0.921 & \underline{117.69} \\
    realisdance & 0.254 & 18.08 & 0.734 & 3051.80 \\
    realisdance微调 & \underline{0.034} & \underline{29.73} & \underline{0.944} & 668.17 \\
    本文模型 & \textbf{0.023} & \textbf{30.66} & \textbf{0.969} & \textbf{22.41} \\ \bottomrule
    \end{tabular}
    }
    \end{table}

\begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.9\textwidth]{./m_figures/chapter5/result/LIA横向对比.pdf}
        \caption{本文模型与realisdance\cite{zhou2024realisdance}、LIA\cite{wang2024lia} 模型结果对比图}
        \label{fig:othermodel_comparison}
    \end{figure}
    
3)\textbf{实验分析}

本文的生成方案具有突出的性能优势，在生成速度、生成分辨率和显存占用都优于现有的泛化性解决方案。尤其有效的解决实时性应用和大规模部署中的难题。其中，对于“时序层”额外的显存占用，本文将需要额外载入显存的连续 $l$ 帧显存占用也计算在内。以“本文模型”方案为例，其生成速度是“realisdance”方案使用 DDIM\cite{ddim} 采样器30步采样时生成速率的 342 倍，是“LIA”方案的 2 倍。基本达到了实时生成的需求。其中“基准模型”方法，能够达到 55 帧每秒的生成速度，而整合本文中所有改进的“本文模型”方案也能达到 36 帧每秒的生成速度。均满足实时性应用的需求。而从显存占用来看，以“本文模型”方案为例，其分辨率相较“LIA”方案提高四倍的情况下，显存占用也仅是“LIA”方案的 4 倍；而相较“realisdance”方案其显存占用仅有 1/8。从显存占用和推理速度的角度，本文所述技术方案均能够满足更大规模的部署要求，实际部署时甚至能直接运行在消费级显卡当中。

在表~\ref{tab:model_performance_comparison}~比较中，“本文模型”方案在所有指标上均优于 “LIA（微调）” 和 “realisdance（微调）” 。“realisdance”方案在微调过后其输出的单帧视频质量指标LPIPS、PSNR、SSIM表现有显著改善，在数值上分别下降了 32\%，提高了 64\%和 28\%。接近“本文模型”水平。但其生成结果时序一致性极差，FVD指标高达 668.17，超过本文模型 30 倍。而“LIA”方案生成结果的时序一致性要显著强于“realisdance”，经过微调后LIA模型时序一致性有了明显提升，FVD指标下降了2倍，但仍超过“本文模型” 5 倍。其LPIPS、PSNR、SSIM 指标相较本文均更差，其指标数值分别提高5.2\%，降低 35.7\%，提高 4\%。


4)\textbf{可视化结果与分析}

图~\ref{fig:othermodel_comparison}~中展示了本文模型与LIA、realisdance模型在测试集上的可视化对比结果。其中前两行分别展示了LIA方法微调前后的模型，三四行分别展示了realisdance微调前后模型输出结果，五六行中分别展示了本文模型和原始视频帧。本节可视化结果与前文定量指标表现一致，开源模型未进行微调时均无法很好拟合人物形象，其中“LIA”方案表现为动作控制失效，可控性差；“realisdance”表现为人物形象异常。经过微调后，以上方案均在可控性和画面质量上得到了提升，但均不如本文提出的技术方案。该实验结果表明了本文的模型能良好的在高分辨下重建对应的数字人形象，相较于其他公开方法具有一定性能优势。

\section{本章小结}

本章提出了基于帧间连续性与法线属性融合的时序一致性改进方案。详细介绍了其中基于三维分组卷积的时序融合方法和基于多任务学习的法线特征融合方法。通过模型消融实验和时序闭环检测实验，证明该章节提出的改进能有效改善数字人生成中的时序一致性问题，并相较于基准模型进一步提升了生成视频的质量。最后通过与其他模型进行横向性能对比，进一步证明本文所提出方案在生成质量、可控性和生成速度上均具有优越性。